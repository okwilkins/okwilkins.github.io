<!doctype html><html lang=en><head><title>Ml & Ai Note Temp :: Oli Wilkins</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="|Yes|23|117| |No|6|210|
Odds ratio: $\frac{23 / 117}{6 / 210} = \frac{0.2}{0.03} = 6.88$ This means that the odds are 6.88 times greater that someone with the mutated gene will also have cancer Hence log ratio: $1.93$
The odds ratio and log of the odds ratio are like R-squared; they indicate a relationship between two things Larger values mean that the mutated gene is a good predictor of cancer Values close to 0 are bad: $log(1) = 0$ $log(0."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://www.okwilkins.dev/knowledge-system/slip-box/ml-ai-note-temp/><link rel=stylesheet href=https://www.okwilkins.dev/assets/style.css><link rel=stylesheet href=https://www.okwilkins.dev/assets/blue.css><link rel=apple-touch-icon href=https://www.okwilkins.dev/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://www.okwilkins.dev/img/favicon/blue.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Ml & Ai Note Temp"><meta property="og:description" content="|Yes|23|117| |No|6|210|
Odds ratio: $\frac{23 / 117}{6 / 210} = \frac{0.2}{0.03} = 6.88$ This means that the odds are 6.88 times greater that someone with the mutated gene will also have cancer Hence log ratio: $1.93$
The odds ratio and log of the odds ratio are like R-squared; they indicate a relationship between two things Larger values mean that the mutated gene is a good predictor of cancer Values close to 0 are bad: $log(1) = 0$ $log(0."><meta property="og:url" content="https://www.okwilkins.dev/knowledge-system/slip-box/ml-ai-note-temp/"><meta property="og:site_name" content="Oli Wilkins"><meta property="og:image" content="https://www.okwilkins.dev/"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"></head><body class=blue><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Oli Wilkins</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/knowledge-system>Knowledge System</a></li><li><a href=/posts>Posts</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/knowledge-system>Knowledge System</a></li><li><a href=/posts>Posts</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://www.okwilkins.dev/knowledge-system/slip-box/ml-ai-note-temp/>Ml & Ai Note Temp</a></h1><div class=post-meta></div><div class=post-content><div><p>|Yes|23|117|
|No|6|210|</p><p>Odds ratio: $\frac{23 / 117}{6 / 210} = \frac{0.2}{0.03} = 6.88$
This means that the odds are 6.88 times greater that someone with the mutated gene will also have cancer
Hence log ratio: $1.93$</p><ul><li>The odds ratio and log of the odds ratio are like R-squared; they indicate a relationship between two things</li><li>Larger values mean that the mutated gene is a good predictor of cancer</li><li>Values close to 0 are bad: $log(1) = 0$<ul><li>$log(0.001) = -6.91$ (so do opposite) (for every game I win someone else wins 1,000 times)</li><li>$log(100) = 4.6$ (for every game I lose, I win 100)</li><li>hence these are good predictors</li></ul></li></ul><p>3 ways to determine if an odds ratio is statistically signficant:</p><ul><li>Fisher&rsquo;s Exact Test</li><li>Chi-Square Test</li><li>The Wald test
No consensus on what is best, so people just mix and match:</li><li>Some people use Fisher&rsquo;s/Chi-Square test to get p-values</li><li>Wald Test to calculate a confidence interval</li><li>Others use just Wald Test for confidence interval/p-values</li><li>So need to check what is best for your field</li></ul><h1 id=logistic-regression>Logistic Regression<a href=#logistic-regression class=hanchor arialabel=Anchor>&#8983;</a></h1><ul><li>Predicts if something is True or False</li><li>Fits an S shaped logistic function</li><li>Test to see if a variable&rsquo;s effect on the prediction is significantly different from 0<ul><li>If not, it means the variable isnot helping the prediction</li><li>This is called Wald&rsquo;s test</li></ul></li><li>Instead of using SSR and R^2 it uses <em><strong>maximum likelihood</strong></em></li><li>Logistic Regression is a specific type of <strong>Generalised Linear Model</strong> (<strong>GLM</strong>)</li></ul><p>Continuous values:
Because with logistic regression, the y-axis is confined to probability values between 0 and 1</p><ul><li>solve problem by transforming to the log odds, so like with linear regression the y-axis can go from $-\infty$ to $+\infty$</li></ul><p>So we use the <strong>logit function</strong>: $\log{\frac{p}{1 - p}}$</p><ul><li>So all values that are true (1) -> $\infty$</li><li>All values that are false (0) -> $-\infty$</li></ul><p>Take example logisitic regression line: $y = -3.48 + 1.83\times weight$
So the first coefficient is the y-axis intercept</p><ul><li>when weight (x-axis) = 0, then log(odds of obesity) = -3.476<ul><li>In other words, if you dont weigh anything, the odds are against you being obese!! Duh!</li></ul></li><li>The slope: for every one unit of weight gains, the log(odds of obesity) increases by 1.825</li></ul><p>Descrete variable:</p><p><em><strong>Need to come back to this (2nd vid for logistic regression)</strong></em></p><h1 id=linear-models-for-t-tests-and-anova>Linear Models for T-Tests and ANOVA<a href=#linear-models-for-t-tests-and-anova class=hanchor arialabel=Anchor>&#8983;</a></h1><p><em><strong>Need to come back to this</strong></em></p><h1 id=gradient-descent>Gradient Descent<a href=#gradient-descent class=hanchor arialabel=Anchor>&#8983;</a></h1><p>Can be used to optimise least squares like in notes above!
For: $y = ax + b$
start with least squares estimate for the slope a
we use gradient descent to find optimatal value for b</p><p>set b = 0, any number will do
calc SSR</p><ul><li>NOTE: the SSR is a type of <strong>loss function</strong></li></ul><p>Keep changing the intercept and plot a graph of intercept vs SSR</p><ul><li>Inefficent way of doing this would be to calc lots and lots of points</li><li>Gradient descent makes this way more efficent<ul><li>Does a few calcs when far from the optimal solution</li><li>increases the number of calcs, close to the optimal value</li></ul></li><li>You can derive an equation for a line that describes the SSR</li><li>Determine derivative of the function</li><li>Then take steps towards min value approaches derivative = 0<ul><li>This is v useful because get around times where it&rsquo;s impossible for derivative = 0</li><li>The size of the step should be dependant on the size of the derivative</li><li>This is determined by a <strong>step size</strong> called the<ul><li>Mutiply the derivative by a small number called a <strong>learning rate</strong>, the combination of these two is the step size</li></ul></li><li>So in this case, the new intercept = old intercept - step size</li><li>This process repeats itself</li><li>It will stop when the step size is very close to 0</li></ul></li></ul><p>How to do grad descent for both a and b?</p><ul><li>Take derivative with respect to the intercept also do the same with respect to the slope<ul><li>When you have 2 or more derivatives of the same function, they are called a <strong>gradient</strong></li><li>This is why the algorithm is called <strong>gradient descent</strong>!</li></ul></li><li>Pick rando number for intecept and slope<ul><li>Gives 2 SSRs</li></ul></li><li>Calc step size for both</li><li>Adjust intercept and slope by step size</li></ul><p>In general:</p><ol><li>Take the derivative of the <strong>Loss Function</strong> for each parameter in it. Meaning take the <strong>Gradient</strong> of the <strong>Loss Function</strong>.</li><li>Pick rando values for all parameters.</li><li>Plug param values into the gradient</li><li>Calc step size</li><li>calculate new params</li><li>Go back to step 3 and repeat till step size small or max num of steps reached</li></ol><p>For a lot of data points, this can take long time
So, <strong>stochastic gradient descent</strong> that uses a rando selected subset of the data at every step, rather than the full dataset</p><h1 id=gradient-boosting-regression-main-ideas>Gradient Boosting: Regression Main Ideas<a href=#gradient-boosting-regression-main-ideas class=hanchor arialabel=Anchor>&#8983;</a></h1><p>Grad boost very similar to AdaBoost</p><p>When starting, grad boost creates a single leaf that is the avg of the target var
Then grad bost builds a tree, that is based on the errors of the previous tree.</p><ul><li>This tree is larger than stump but still restricts the size of the tree</li><li>In practice, people ofen set the num of leaves to be between 8 and 32</li></ul><p>Grad boost will scale each tree by a fixed amount, unlike AdaBoost (amount of say)
It will then build tree based of the errors of the previous tree.
It will then build trees in this fashon till made num of trees asked for or aditional trees fail to improve fit.</p><ol><li>Calc avg of the target feature as first &rsquo;tree&rsquo;/leaf<ol><li>Better way: $F_{0}(x) = \arg \max_{\gamma}\sum_{i=1}^{n}L(y_{i}, \gamma)$, $L$ is loss func</li><li>Either find best value via grad descent or if ez derivative to solve for 0, do that</li></ol></li><li>Build tree on errors of first tree: error = obvs - pred, this is called a <strong>pseudo residual</strong>. The pseudo part is a reminder that we are doing grad boost, not linear reg.</li><li>Now use input vars to predict the residuals to build tree. A leaf of this tree may pointwards two residuals. If this happens, get avg of residuals on leaf.</li><li>Now combine original leaf with the new tree for pred. Effectively this new tree &lsquo;corrects&rsquo; for the error of the og leaf. After doing this once, the bias will be low but varience high (overfit), so there is a learning rate to correct for this. Multiple learning rate by new tree&rsquo;s residual pred. Between 0 and 1. This results in a small step in the right direction. Taking lots of small steps in the right direction results in lower varience.</li><li>Calc psuedo residuals again based of new preds and do the same.</li><li>Keep doing this until num trees reached or the sum of residuals do not significantly decrease.</li></ol><p>Gradient boost is called gradient boost because the residual is the <strong>Gradient</strong> that comes from <strong>Gradient descent</strong>.</p><h1 id=gradient-boost-classification-main-ideas>Gradient Boost: Classification Main Ideas<a href=#gradient-boost-classification-main-ideas class=hanchor arialabel=Anchor>&#8983;</a></h1><p><em><strong>Need to come back to this!</strong></em></p><h1 id=regularization-ridge-l2-regression>Regularization: Ridge (L2) Regression<a href=#regularization-ridge-l2-regression class=hanchor arialabel=Anchor>&#8983;</a></h1><ul><li>Another way of saying desensitisation (lol)</li></ul><p>Use Linear regression aka least squares:</p><ul><li>example: has intercept and slope<ul><li>training data has 2 points</li><li>testing has 8</li><li>SSR = 0 for training, for testing is high<ul><li>high varience (overfit to training)</li></ul></li></ul></li><li>Main idea behind ridge regression is to find a new line that doesn&rsquo;t fit the training data as well<ul><li>introduce a small amount of bias</li><li>in return we get a significant drop in variance</li></ul></li></ul><p>Example: size = intercept + slope * Weight
Lin reg minises: SSR
Ridge regression tries to minimises: SSR + $\lambda \times {slope}^2$</p><ul><li>The added tern adds a penalty to the traditional least squares method</li></ul><p>So when the slope of the line is steep, then the prediction for size is very sensitive to relatively small changes in weight
So predictions made with the ridge regression line are less sensistive to weight than the least squares line
The large lambda, the slope get asymptotically close to 0
To find the value for lambda, try typically 10-fold cross validation to determine which one results in the lowest variance</p><p>Also works on descrete data:
size = 1.5 + 0.7 * high fat diet</p><p>so ridge regression minimises: SSR + $\lambda \times {diet difference}^2$</p><ul><li>diet difference refers to the distance in size between normal diet and high far diet</li></ul><p>for more complex model:
size = intercept + slope * weight + diet difference * high fat diet
so penalty term is: lambda * (slope ^ 2 + diet difference ^ 2)</p><p>so in general, the penalty term will apply itself to every parameter, except for the intercept, is scaled by the measurements</p><p>you cant make a line with one data point
nor a plane with 2 data points</p><ul><li>you would need at least 10,001 data points to make an equation of 10,001 parameters,<ul><li>sometimes that&rsquo;s not possible</li></ul></li><li>what do you do here?</li></ul><p>use ridge regression!</p><ul><li>The penalty can solve for all 10,001 params with 500 samples or fewer</li></ul><h1 id=regularization-lasso-l1-regression>Regularization: Lasso (L1) Regression<a href=#regularization-lasso-l1-regression class=hanchor arialabel=Anchor>&#8983;</a></h1><p>Very similar to ridge regression but has some important differences
Intead the penalty is: $\lambda \times |slope|$</p><ul><li>adds a bit of bias but less variarience than leat squares</li></ul><p>The big difference is that L1 can make the slope go to 0</p><ul><li>Ridge can only do this asymptotically</li></ul><p>This can eleminate any terms in an equation that are stupid to 0</p><p>So lasso does better when there are lots of useless parameters
Ridge does better then most of the variables are useful</p><h1 id=xgboost-regression>XGBoost: Regression<a href=#xgboost-regression class=hanchor arialabel=Anchor>&#8983;</a></h1><p>Has lots of parts but are simple:</p><ol><li>Gradient boost(ish) (wont be covered as done before)</li><li>Regularization (wont be covered as done before)</li><li>A unique regression tree</li><li>Approximate greedy algorithm</li><li>Weighted quantile sketch</li><li>Sparsity-aware split finding</li><li>Parallel learning</li><li>Cache-aware access</li><li>Blocks for out-of-core computation</li></ol><p>XGBoost was designed to be used with large, complicated datasets</p><h2 id=unique-regression-tree>Unique Regression Tree<a href=#unique-regression-tree class=hanchor arialabel=Anchor>&#8983;</a></h2><p>First step in fitting XGBoost to the training data is to make an initial prediction</p><ul><li>This prediction can be anything but by default it is 0.5, regardless if it&rsquo;s regression or classification</li></ul><p>Like with gradient boost, XGBoost fits a regression tree to the residuals</p><ul><li>However, XGBoost uses a unique regression tree that the vid calls XGBoost tree
There are many ways this tree can be built, but this is the most common way for regression</li></ul><ol><li>Each tree starts with a single leaf, all residuals go to the leaf</li><li>Calculate a quality score or similarity score for the residuals<ol><li>Similarity score = SSR / (num residuals + $\lambda$)</li><li>This similarity score is a simplification of second order Taylor approximation of the loss function described in grad boost</li><li>Also, the lambda here will just be for L2 regularisation</li><li>L1 will make the sim score simplify to something different</li></ol></li><li>Now the question is whether or not we can do a better job clustering similar residuals if we split them into two groups<ol><li>Focus on the two points that are smallest (x-axis)</li><li>Get avg value</li><li>Make a split based off the avg value</li><li>Calc similarity score for both leafs</li></ol></li><li>Now need to quanitify how much better the leaves cluster similar resdiuals than the root, do this by calculating the gain<ol><li>$gain = {left}<em>{similarity} + {right}</em>{similarity} - {root}_{similarity}$</li></ol></li><li>Now shift the threshold over so that it is the avg of th next two observation<ol><li>More gain is good, is better at splitting the residuals into clusters of similar values</li></ol></li><li>Get best tree based of thresholds</li><li>Try to split leaves further, the root in the gain calc refers to the parent node of the leaves</li><li>Keep going till at max depth (default is 6)</li></ol><p>How to prune this tree?</p><ul><li>Pruned based on its gain values</li><li>Start with a number, for example, 130<ul><li>This is called $\gamma$ gamma</li><li>Then calc the difference between the gain asscociated with the lowest branch in the tree and gamma: $gain - \gamma$<ul><li>if the difference is positive then the branch is not removed, if it is neg then remove</li><li>once you find a pos difference, then stop pruning</li></ul></li><li>This can also remove the root of the tree, leaving just the original prediction, which is pretty extreme pruning</li></ul></li></ul><p>The regularization term:</p><ul><li>This will decrease similarity scores</li><li>This will tend to stop the trees getting so large as it&rsquo;s harder for a branch to have a lower sim score than its parent</li><li>This also can make the pruning more agressive as $\gamma$ is more likely to be larger than the gain</li><li>In the case of gain being less than 0, setting gamma = 0 will not turn off pruning</li></ul><p>Output value = sum of residuals / (num of residuals + $\lambda$)</p><ul><li>This refers to the output value for each leaf</li><li>Pretty similar to the similarity score except it&rsquo;s not SSR</li><li>This also means that lambda will reduce the amount a leaf will contribute to a prediction<ul><li>So will reduce the sensistivity to an indivual observation</li></ul></li></ul><p>Like with gradient boost, there is a learning rate that scales the tree</p><ul><li>XGboost calls this learning rate: $\epsilon$ (eta but this is epsilon?)<ul><li>Default value is 0.3</li></ul></li></ul><p>So output = inital pred + learning rate * output value for the leaf that the point goes down to</p><p>Now build new tree based on the new residuals</p><p>The two reasons to use XGBoost are also the two goals of the project:</p><ul><li>Execution Speed</li><li>Model Performance</li></ul><p><strong>Sparse Aware</strong>: implementation with automatic handling of missing data values.
<strong>Block Structure</strong>: to support the parallelization of tree construction.
<strong>Continued Training</strong>: so that you can further boost an already fitted model on new data.</p><h2 id=approximate-greedy-algorithm>Approximate Greedy Algorithm<a href=#approximate-greedy-algorithm class=hanchor arialabel=Anchor>&#8983;</a></h2><p>When fitting a tree to the residuals</p><ul><li>was done by calculating the similarity scores and the gain for each possible threshold</li><li>the threshold with the largest gain is the one XGBoost uses</li><li>The decision to use the threshold that gives the largest gain is made without worrying about how the leaves will be split later</li><li>This means XGBoost uses a greedy algorithm</li><li>In other words, since XGBoost uses a greedy algorithm, it amkes a decision without looking ahead to see if it is the absolute best choice in the long term</li><li>If it did not use this algo, it woukld postpone making a final decision about this threhold, until after trying different thresholds in the leaves to see how things played out in the long run</li><li>So, the greedy algo makes XGBoost a tree relatively quickly</li></ul><p>When you have lots of data points, the greedy algo still has to look through every possible threshold</p><ul><li>on top of that, you&rsquo;d have to do this for every other feature in your data</li><li>this would take forever</li><li>this is where the approximate greedy algo comes in</li></ul><p>Instead, divide the data into quantiles and use that for thresholds instead
By default, XGBoost uses &ldquo;about&rdquo; 33 quantiles</p><ul><li>The &ldquo;about&rdquo; comes from the Parallel Learning an Weighted Quantile Sketch</li></ul><h2 id=parallel-learning--weighted-quantile-sketch>Parallel Learning & Weighted Quantile Sketch<a href=#parallel-learning--weighted-quantile-sketch class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Get your data and split it into small pieces and putting the peices on different computers on a network
The quantile sketch algorithm combines the alues form each computer to make an approximate histogram
Then the approximate histogram is used to calculate approximate quantiles
The Approximate greedy algorithm uses these approximate quantiles</p><p>What about the weighted part?</p><p>Usually, quantiles are set up so that the same number of observations are in each one</p><ul><li>In contrast, with weighted quantiles, each observation has a corresponding weight</li><li>The sum of the weights are the same in each quantile</li><li>The weights are derived from the cover metric<ul><li>the weight for each observation is the 2nd derivative of the loss function, what we are referring to as the Hessian (not Gradient)</li></ul></li><li>This means for regression the weights are all equal to 1<ul><li>This means that the the weighted quantiles are just like normal quantiles and contain an equal number of observations</li></ul></li><li>In constrast, for classification: weight = previous prob i * (1- previous prob i)</li></ul><p>Only uses the approx greedy algo, parallel learning and the weighted quantile sketch then the training data is huge</p><p>When the trainin set is not so large, a normal greedy algo is used</p><h2 id=sparsity-aware-split-finding>Sparsity-Aware Split Finding<a href=#sparsity-aware-split-finding class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Have a few missing values</p><ul><li>Even with this, we can just use the base leaf to calculate the residuals for rows with missing data</li></ul><p>split the data into two table</p><ul><li>one with all e.g. dosage values</li><li>other with all values without dosage values</li></ul><p>Table with dosage values:</p><ul><li>sort rows low to high</li><li>calc candidate thresholds</li></ul><p>The first gain value, gain left, is calculated by putting all of the resdiuals with missing dosage values into the leaf on the left, save gain
Do the same for leaf on the right, save gain</p><p>Do this for all candidate thresholds
You then pick the tree with highest gain</p><p>So example, Dosage &lt; 15.5, going left will be the default path for all future observations that are missing dosage values</p><h2 id=cache-aware-access>Cache-Aware Access<a href=#cache-aware-access class=hanchor arialabel=Anchor>&#8983;</a></h2><p>This is where XGBoost starts to get super nitty gritty</p><p>Basic idea:</p><ul><li>Inside computer we have:<ul><li>CPU</li><li>CPU has a small amount of cache memory &ndash; CPU can use this mem faster than any other memory on the computer</li><li>CPU attatched to large amount of main memory, larger but slower than cache</li><li>HDD, very slow but largest</li></ul></li></ul><p>XGBoost puts the gradients and Hessians in the cache, so that is can rapdily calculate similarity scores and output scores</p><h2 id=blocks-for-out-of-core-computation>Blocks for Out-of-Core Computation<a href=#blocks-for-out-of-core-computation class=hanchor arialabel=Anchor>&#8983;</a></h2><p>When the dataset is too large for the cache and main memory, then some of it must be stored on the HDD</p><p>Because reading and writing data to the HDD is super slow, XGBoost tries minimising these actions by compressing the data</p><p>When there is more than one HDD, XGBoost uses a databse technique called sharding to speed up disk access</p><ul><li>The when the CPU needs data, both drives can be reading data at the same time</li></ul><hr><p>Finally, XGHBoost can speed things up by allowing you to build each tree with only a random subset of the data.</p><p>AND, can build trees by only looking at a random subset of features when deciding how to split the data.</p></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2023 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://www.okwilkins.dev/assets/main.js></script>
<script src=https://www.okwilkins.dev/assets/prism.js></script></div></body></html>