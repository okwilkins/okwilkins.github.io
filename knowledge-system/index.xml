<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Knowledge-systems on Oli Wilkins</title><link>https://www.okwilkins.dev/knowledge-system/</link><description>Recent content in Knowledge-systems on Oli Wilkins</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Mon, 27 Feb 2023 12:37:35 +0000</lastBuildDate><atom:link href="https://www.okwilkins.dev/knowledge-system/index.xml" rel="self" type="application/rss+xml"/><item><title>Mermaid</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid/</guid><description>Mermaid uses Javascript to create diagrams and visualisations via text and code. The text and code are Markdown inspired that modify diagrams dynamically.
References Mermaid#About Mermaid</description><content>&lt;p>Mermaid uses &lt;em>Javascript&lt;/em> to create diagrams and visualisations via text and code. The text and code are &lt;em>Markdown&lt;/em> inspired that modify diagrams dynamically.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#About Mermaid&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Class Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-class-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-class-diagram/</guid><description>classDiagram Class01 &amp;lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --&amp;gt; C2 : Where am i? Class09 --* C3 Class09 --|&amp;gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 &amp;lt;--&amp;gt; C2: Cool label classDiagram Class01 &lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --> C2 : Where am i?</description><content>&lt;pre tabindex="0">&lt;code>classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
Class08 &amp;lt;--&amp;gt; C2: Cool label
&lt;/code>&lt;/pre>&lt;div class="mermaid">classDiagram
Class01 &lt;|-- AveryLongClass : Cool
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 --> C2 : Where am i?
Class09 --* C3
Class09 --|> Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
Class08 &lt;--> C2: Cool label
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Class Diagrams&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Entity Relationship Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-entity-relationship-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-entity-relationship-diagram/</guid><description>erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses References Mermaid#Entity Relationship Diagrams</description><content>&lt;pre tabindex="0">&lt;code>erDiagram
CUSTOMER ||--o{ ORDER : places
ORDER ||--|{ LINE-ITEM : contains
CUSTOMER }|..|{ DELIVERY-ADDRESS : uses
&lt;/code>&lt;/pre>&lt;div class="mermaid">erDiagram
CUSTOMER ||--o{ ORDER : places
ORDER ||--|{ LINE-ITEM : contains
CUSTOMER }|..|{ DELIVERY-ADDRESS : uses
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Entity Relationship Diagrams&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Flowchart</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-flowchart/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-flowchart/</guid><description>graph TD; A--&amp;gt;B; A--&amp;gt;C; B--&amp;gt;D; C--&amp;gt;D; graph TD; A-->B; A-->C; B-->D; C-->D; References Mermaid#Flowchart</description><content>&lt;pre tabindex="0">&lt;code>graph TD;
A--&amp;gt;B;
A--&amp;gt;C;
B--&amp;gt;D;
C--&amp;gt;D;
&lt;/code>&lt;/pre>&lt;div class="mermaid">graph TD;
A-->B;
A-->C;
B-->D;
C-->D;
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Flowchart&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Gantt Chart</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-gantt-chart/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-gantt-chart/</guid><description>gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d References Mermaid#Gantt Charts</description><content>&lt;pre tabindex="0">&lt;code>gantt
dateFormat YYYY-MM-DD
title Adding GANTT diagram to mermaid
excludes weekdays 2014-01-10
section A section
Completed task :done, des1, 2014-01-06,2014-01-08
Active task :active, des2, 2014-01-09, 3d
Future task : des3, after des2, 5d
Future task2 : des4, after des3, 5d
&lt;/code>&lt;/pre>&lt;div class="mermaid">gantt
dateFormat YYYY-MM-DD
title Adding GANTT diagram to mermaid
excludes weekdays 2014-01-10
section A section
Completed task :done, des1, 2014-01-06,2014-01-08
Active task :active, des2, 2014-01-09, 3d
Future task : des3, after des2, 5d
Future task2 : des4, after des3, 5d
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Gantt Charts&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Gitgraph Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-gitgraph-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-gitgraph-diagram/</guid><description>gitGraph commit commit branch develop commit commit commit checkout main commit commit gitGraph commit commit branch develop commit commit commit checkout main commit commit References Mermaid#Gitgraph Diagrams</description><content>&lt;pre tabindex="0">&lt;code>gitGraph
commit
commit
branch develop
commit
commit
commit
checkout main
commit
commit
&lt;/code>&lt;/pre>&lt;div class="mermaid">gitGraph
commit
commit
branch develop
commit
commit
commit
checkout main
commit
commit
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Gitgraph Diagrams&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Mindmap</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-mindmap/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-mindmap/</guid><description>mindmap root((mindmap)) Origins Long history ::icon(fa fa-book) Popularisation British popular psychology author Tony Buzan Research On effectiveness&amp;lt;br/&amp;gt;and features On Automatic creation Uses Creative techniques Strategic planning Argument mapping Tools Pen and paper Mermaid mindmap root((mindmap)) Origins Long history ::icon(fa fa-book) Popularisation British popular psychology author Tony Buzan Research On effectivenessand features On Automatic creation Uses Creative techniques Strategic planning Argument mapping Tools Pen and paper Mermaid References Mermaid#Mindmap</description><content>&lt;pre tabindex="0">&lt;code>mindmap
root((mindmap))
Origins
Long history
::icon(fa fa-book)
Popularisation
British popular psychology author Tony Buzan
Research
On effectiveness&amp;lt;br/&amp;gt;and features
On Automatic creation
Uses
Creative techniques
Strategic planning
Argument mapping
Tools
Pen and paper
Mermaid
&lt;/code>&lt;/pre>&lt;div class="mermaid">mindmap
root((mindmap))
Origins
Long history
::icon(fa fa-book)
Popularisation
British popular psychology author Tony Buzan
Research
On effectiveness&lt;br/>and features
On Automatic creation
Uses
Creative techniques
Strategic planning
Argument mapping
Tools
Pen and paper
Mermaid
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Mindmap&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Pie Chart</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-pie-chart/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-pie-chart/</guid><description>Start with pie keyword to begin the diagram showData to render the actual data values after the legend text. This is OPTIONAL Followed by title keyword and its value in string to give a title to the pie-chart. This is OPTIONAL Followed by dataSet. Pie slices will be ordered clockwise in the same order as the labels. label for a section in the pie diagram within &amp;quot; &amp;quot; quotes. Followed by : colon as separator Followed by positive numeric value (supported up to two decimal places) pie showData title Key elements in Product X &amp;#34;Calcium&amp;#34; : 42.</description><content>&lt;ul>
&lt;li>Start with &lt;code>pie&lt;/code> keyword to begin the diagram&lt;/li>
&lt;li>&lt;code>showData&lt;/code> to render the actual data values after the legend text. This is &lt;em>&lt;strong>OPTIONAL&lt;/strong>&lt;/em>&lt;/li>
&lt;li>Followed by &lt;code>title&lt;/code> keyword and its value in string to give a title to the pie-chart. This is &lt;em>&lt;strong>OPTIONAL&lt;/strong>&lt;/em>&lt;/li>
&lt;li>Followed by dataSet. Pie slices will be ordered clockwise in the same order as the labels.&lt;/li>
&lt;li>&lt;code>label&lt;/code> for a section in the pie diagram within &lt;code>&amp;quot; &amp;quot;&lt;/code> quotes.&lt;/li>
&lt;li>Followed by &lt;code>:&lt;/code> colon as separator&lt;/li>
&lt;li>Followed by &lt;code>positive numeric value&lt;/code> (supported up to two decimal places)&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>pie showData
title Key elements in Product X
&amp;#34;Calcium&amp;#34; : 42.96
&amp;#34;Potassium&amp;#34; : 50.05
&amp;#34;Magnesium&amp;#34; : 10.01
&amp;#34;Iron&amp;#34; : 5
&lt;/code>&lt;/pre>&lt;div class="mermaid">pie showData
title Key elements in Product X
"Calcium" : 42.96
"Potassium" : 50.05
"Magnesium" : 10.01
"Iron" : 5
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Pie Charts&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Sequence Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-sequence-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-sequence-diagram/</guid><description>sequenceDiagram participant Alice participant Bob Alice-&amp;gt;&amp;gt;John: Hello John, how are you? loop Healthcheck John-&amp;gt;&amp;gt;John: Fight against hypochondria end Note right of John: Rational thoughts &amp;lt;br/&amp;gt;prevail! John--&amp;gt;&amp;gt;Alice: Great! John-&amp;gt;&amp;gt;Bob: How about you? Bob--&amp;gt;&amp;gt;John: Jolly good! sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! References Mermaid#Sequence Diagrams</description><content>&lt;pre tabindex="0">&lt;code>sequenceDiagram
participant Alice
participant Bob
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts &amp;lt;br/&amp;gt;prevail!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
&lt;/code>&lt;/pre>&lt;div class="mermaid">sequenceDiagram
participant Alice
participant Bob
Alice->>John: Hello John, how are you?
loop Healthcheck
John->>John: Fight against hypochondria
end
Note right of John: Rational thoughts &lt;br/>prevail!
John-->>Alice: Great!
John->>Bob: How about you?
Bob-->>John: Jolly good!
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#Sequence Diagrams&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid State Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-state-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-state-diagram/</guid><description>stateDiagram [*] --&amp;gt; Still Still --&amp;gt; [*] Still --&amp;gt; Moving Moving --&amp;gt; Still Moving --&amp;gt; Crash Crash --&amp;gt; [*] stateDiagram [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] References Mermaid#State Diagrams</description><content>&lt;pre tabindex="0">&lt;code>stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
&lt;/code>&lt;/pre>&lt;div class="mermaid">stateDiagram
[*] --> Still
Still --> [*]
Still --> Moving
Moving --> Still
Moving --> Crash
Crash --> [*]
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#State Diagrams&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Mermaid Timeline Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-timeline-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-timeline-diagram/</guid><description>timeline title History of Social Media Platform 2002 : LinkedIn 2004 : Facebook : Google 2005 : Youtube 2006 : Twitter timeline title History of Social Media Platform 2002 : LinkedIn 2004 : Facebook : Google 2005 : Youtube 2006 : Twitter References</description><content>&lt;pre tabindex="0">&lt;code>timeline
title History of Social Media Platform
2002 : LinkedIn
2004 : Facebook
: Google
2005 : Youtube
2006 : Twitter
&lt;/code>&lt;/pre>&lt;div class="mermaid">timeline
title History of Social Media Platform
2002 : LinkedIn
2004 : Facebook
: Google
2005 : Youtube
2006 : Twitter
&lt;/div>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Mermaid User Journey Diagram</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-user-journey-diagram/</link><pubDate>Mon, 27 Feb 2023 12:37:35 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-user-journey-diagram/</guid><description>journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me References Mermaid#User Journey Diagrams</description><content>&lt;pre tabindex="0">&lt;code>journey
title My working day
section Go to work
Make tea: 5: Me
Go upstairs: 3: Me
Do work: 1: Me, Cat
section Go home
Go downstairs: 5: Me
Sit down: 5: Me
&lt;/code>&lt;/pre>&lt;div class="mermaid">journey
title My working day
section Go to work
Make tea: 5: Me
Go upstairs: 3: Me
Do work: 1: Me, Cat
section Go home
Go downstairs: 5: Me
Sit down: 5: Me
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/mermaid/">Mermaid#User Journey Diagrams&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Decision Trees</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/decision-trees/</link><pubDate>Mon, 27 Feb 2023 11:57:32 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/decision-trees/</guid><description>A decision tree is a tree of statements and with decisions based on whether or not the statement is true or false.
flowchart TD A[A person wants to learn about decision trees] A --> |True| B[Check out these sequence of notes!] A --> |False| C[Go somewhere else then!] If the statement is true then you go left and vice versa.
References StatQuest with Josh Starmer &amp;gt; Decision Trees</description><content>&lt;p>A decision tree is a tree of statements and with decisions based on whether or not the statement is &lt;strong>true&lt;/strong> or &lt;strong>false&lt;/strong>.&lt;/p>
&lt;div class="mermaid">flowchart TD
A[A person wants to learn about decision trees]
A --> |True| B[Check out these sequence of notes!]
A --> |False| C[Go somewhere else then!]
&lt;/div>
&lt;p>If the statement is &lt;strong>true&lt;/strong> then you go left and &lt;em>vice versa&lt;/em>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/statquest-with-josh-starmer/#decision-trees">StatQuest with Josh Starmer &amp;gt; Decision Trees&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Machine Learning Variance</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/machine-learning-variance/</link><pubDate>Mon, 27 Feb 2023 11:52:17 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/machine-learning-variance/</guid><description>The difference in how well a model fits differing datasets is called variance.
References StatQuest with Josh Starmer &amp;gt; Variance</description><content>&lt;p>The difference in how well a model fits differing datasets is called &lt;em>&lt;strong>variance&lt;/strong>&lt;/em>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/statquest-with-josh-starmer/#variance">StatQuest with Josh Starmer &amp;gt; Variance&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Machine Learning Bias</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/machine-learning-bias/</link><pubDate>Mon, 27 Feb 2023 11:33:26 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/machine-learning-bias/</guid><description>The inability for a machine learning model to capture the true relationship of a pair of variables is called bias.
References StatQuest with Josh Starmer &amp;gt; Bias</description><content>&lt;p>The inability for a &lt;em>machine learning&lt;/em> model to capture the true relationship of a pair of variables is called &lt;em>&lt;strong>bias&lt;/strong>&lt;/em>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/references/statquest-with-josh-starmer/#bias">StatQuest with Josh Starmer &amp;gt; Bias&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Bulgarian Definite Articles</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-definite-articles/</link><pubDate>Mon, 27 Feb 2023 10:22:34 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-definite-articles/</guid><description>In English, a specific cat would be referred by the following: the cat. In Bulgarian, a definite article is attached to the end of a noun:
Котка - a cat
Котката - the cat
The article will vary depending on the gender and ending of word:
Ending Article Examples Masculine nouns -ът/-а* мъж ➜ мъжът / мъжа* Feminine nouns -та котка ➜ котката, нощ ➜ нощта Neuter nouns -то кафе ➜ кафето Plural nouns ending in -а or -я -та кафета ➜ кафетата All other plural nouns -те градове ➜ градовете References</description><content>&lt;p>In English, a specific cat would be referred by the following: &lt;em>the cat&lt;/em>.
In Bulgarian, a definite article is attached to the end of a &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">noun&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Котка - a cat&lt;/p>
&lt;p>Котката - the cat&lt;/p>
&lt;/blockquote>
&lt;p>The article will vary depending on the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-grammatical-gender/">gender&lt;/a> and ending of word:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Ending&lt;/th>
&lt;th>Article&lt;/th>
&lt;th>Examples&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Masculine nouns&lt;/td>
&lt;td>-ът/-а*&lt;/td>
&lt;td>мъж ➜ мъжът / мъжа*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Feminine nouns&lt;/td>
&lt;td>-та&lt;/td>
&lt;td>котка ➜ котката, нощ ➜ нощта&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Neuter nouns&lt;/td>
&lt;td>-то&lt;/td>
&lt;td>кафе ➜ кафето&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Plural nouns ending in -а or -я&lt;/td>
&lt;td>-та&lt;/td>
&lt;td>кафета ➜ кафетата&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>All other plural nouns&lt;/td>
&lt;td>-те&lt;/td>
&lt;td>градове ➜ градовете&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Masculine Definite Articles</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-masculine-definite-articles/</link><pubDate>Mon, 27 Feb 2023 10:22:34 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-masculine-definite-articles/</guid><description>There are two different definite articles for masculine words. Words ending in a constant will use the -ът article when they are the subject of the sentence. Words will end with -а when they are the object.
Examples:
&amp;ldquo;Мъжът говори&amp;rdquo; - The man is talking
&amp;ldquo;Аз говоря с мъжа&amp;rdquo; - I&amp;rsquo;m talking with the man
References</description><content>&lt;p>There are two different &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/definite-article/">definite articles&lt;/a> for masculine words. Words ending in a constant will use the -ът article when they are the subject of the sentence. Words will end with -а when they are the object.&lt;/p>
&lt;p>Examples:&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;Мъж&lt;strong>ът&lt;/strong> говори&amp;rdquo; - The man is talking&lt;/p>
&lt;p>&amp;ldquo;Аз говоря с мъж&lt;strong>а&lt;/strong>&amp;rdquo; - I&amp;rsquo;m talking with the man&lt;/p>
&lt;/blockquote>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Ml &amp; Ai</title><link>https://www.okwilkins.dev/knowledge-system/projects/ml-ai/</link><pubDate>Wed, 15 Feb 2023 23:05:08 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/projects/ml-ai/</guid><description>General Linear Models
https://www.youtube.com/watch?v=nk2CQITm_eo https://www.youtube.com/watch?v=zITIFTsivN8 https://www.youtube.com/watch?v=NF5_btOaCig (Linear Models for T-Tests and ANOVA) https://youtu.be/2UYx-qjJGSs Least Squares Linear Regression
https://www.youtube.com/watch?v=PaFPbb66DxQ Odds and Log(Odds)
https://www.youtube.com/watch?v=ARfXDSkQf1Y https://www.youtube.com/watch?v=8nm0G-1uJzA Maximum Likelihood
https://www.youtube.com/watch?v=XepXtl9YKwc Logistic Regression
https://www.youtube.com/watch?v=yIYKR4sgzI8 https://www.youtube.com/watch?v=vN5cNN2-HW https://www.youtube.com/watch?v=BfKanl1aSG0 Decision Trees
https://www.youtube.com/watch?v=_L39rN6gz7 https://www.youtube.com/watch?v=g9c66TUylZ4 https://www.youtube.com/watch?v=D0efHEJsfHo Random Forests
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ AdaBoost
https://www.youtube.com/watch?v=LsK-xG1cLYA Gradient Boosting
https://www.youtube.com/watch?v=3CC4N4z3GJ https://www.youtube.com/watch?v=2xudPOBz-vs https://www.youtube.com/watch?v=jxuNLH5dXCs https://www.youtube.com/watch?v=StWY5QWMXCw Regularization
https://www.youtube.com/watch?v=Q81RR3yKn30 https://www.youtube.com/watch?v=NGf0voTMlcs XGBoost
https://www.youtube.com/watch?v=OtD8wVaFm6E https://www.youtube.com/watch?v=8b1JEDvenQU https://www.youtube.com/watch?v=ZVFeW798-2I https://www.youtube.com/watch?v=oRrKeUCEb Decision Trees Basic Concepts milti data type ok a question on a feature can be asked multiple times numerica thresholds can be diff for the same feature final classificaitons can be repeated if statement is true, you go left very top of tree is called root node/the root internal nodes/branches are inbetween top and bottom branches have arrows pointing to them AND arrows pointing away from them leaves have arrows pointing to them but no pointing away Building a Tree with Gini Impurity First need to work out which feature makes the root do this based off which best predicts target var do this by making tree based off only &amp;ldquo;loves popcorn&amp;rdquo; to predict &amp;ldquo;Loves Song&amp;rdquo; If a leaf contains a mixture of correct AND incorrect predictions, then they are IMPURE To quantify the impurity of leaves you can use: Gini Impurity, Entropy, and Information Gain Gini Impurity for a leaf = 1 - (prob(&amp;ldquo;yes&amp;rdquo;) ^ 2 - prob(&amp;ldquo;no&amp;rdquo;) ^ 2) to work out toal gini impurity fpr the simple tree: = weigthed avg of gini impurities weigthed because a leaf may not represent the same number of people eg &amp;ldquo;loves popcorn?</description><content>&lt;p>General Linear Models&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=nk2CQITm_eo">https://www.youtube.com/watch?v=nk2CQITm_eo&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=zITIFTsivN8">https://www.youtube.com/watch?v=zITIFTsivN8&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=NF5_btOaCig">https://www.youtube.com/watch?v=NF5_btOaCig&lt;/a> (Linear Models for T-Tests and ANOVA)&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://youtu.be/2UYx-qjJGSs">https://youtu.be/2UYx-qjJGSs&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Least Squares Linear Regression&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=PaFPbb66DxQ">https://www.youtube.com/watch?v=PaFPbb66DxQ&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Odds and Log(Odds)&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=ARfXDSkQf1Y">https://www.youtube.com/watch?v=ARfXDSkQf1Y&lt;/a>
&lt;a href="https://www.youtube.com/watch?v=8nm0G-1uJzA">https://www.youtube.com/watch?v=8nm0G-1uJzA&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Maximum Likelihood&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=XepXtl9YKwc">https://www.youtube.com/watch?v=XepXtl9YKwc&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Logistic Regression&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=yIYKR4sgzI8">https://www.youtube.com/watch?v=yIYKR4sgzI8&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=vN5cNN2-HW">https://www.youtube.com/watch?v=vN5cNN2-HW&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=BfKanl1aSG0">https://www.youtube.com/watch?v=BfKanl1aSG0&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Decision Trees&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=">https://www.youtube.com/watch?v=&lt;/a>_L39rN6gz7&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=g9c66TUylZ4">https://www.youtube.com/watch?v=g9c66TUylZ4&lt;/a>&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=D0efHEJsfHo">https://www.youtube.com/watch?v=D0efHEJsfHo&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Random Forests&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ">https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>AdaBoost&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=LsK-xG1cLYA">https://www.youtube.com/watch?v=LsK-xG1cLYA&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Gradient Boosting&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=3CC4N4z3GJ">https://www.youtube.com/watch?v=3CC4N4z3GJ&lt;/a>&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=2xudPOBz-vs">https://www.youtube.com/watch?v=2xudPOBz-vs&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=jxuNLH5dXCs">https://www.youtube.com/watch?v=jxuNLH5dXCs&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=StWY5QWMXCw">https://www.youtube.com/watch?v=StWY5QWMXCw&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Regularization&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=Q81RR3yKn30">https://www.youtube.com/watch?v=Q81RR3yKn30&lt;/a>&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=NGf0voTMlcs">https://www.youtube.com/watch?v=NGf0voTMlcs&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>XGBoost&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=OtD8wVaFm6E">https://www.youtube.com/watch?v=OtD8wVaFm6E&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=8b1JEDvenQU">https://www.youtube.com/watch?v=8b1JEDvenQU&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=ZVFeW798-2I">https://www.youtube.com/watch?v=ZVFeW798-2I&lt;/a>&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://www.youtube.com/watch?v=oRrKeUCEb">https://www.youtube.com/watch?v=oRrKeUCEb&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="decision-trees">Decision Trees&lt;/h1>
&lt;h2 id="basic-concepts">Basic Concepts&lt;/h2>
&lt;ul>
&lt;li>milti data type ok&lt;/li>
&lt;li>a question on a feature can be asked multiple times&lt;/li>
&lt;li>numerica thresholds can be diff for the same feature&lt;/li>
&lt;li>final classificaitons can be repeated&lt;/li>
&lt;li>if statement is true, you go left&lt;/li>
&lt;li>very top of tree is called root node/the root&lt;/li>
&lt;li>internal nodes/branches are inbetween top and bottom&lt;/li>
&lt;li>branches have arrows pointing to them&lt;/li>
&lt;li>AND arrows pointing away from them&lt;/li>
&lt;li>leaves have arrows pointing to them but no pointing away&lt;/li>
&lt;/ul>
&lt;h2 id="building-a-tree-with-gini-impurity">Building a Tree with Gini Impurity&lt;/h2>
&lt;ul>
&lt;li>First need to work out which feature makes the root&lt;/li>
&lt;li>do this based off which best predicts target var&lt;/li>
&lt;li>do this by making tree based off only &amp;ldquo;loves popcorn&amp;rdquo; to predict &amp;ldquo;Loves Song&amp;rdquo;&lt;/li>
&lt;li>If a &lt;strong>leaf&lt;/strong> contains a mixture of correct &lt;strong>AND&lt;/strong> incorrect predictions, then they are &lt;strong>IMPURE&lt;/strong>&lt;/li>
&lt;li>To quantify the impurity of leaves you can use: &lt;strong>Gini Impurity&lt;/strong>, &lt;strong>Entropy&lt;/strong>, and &lt;strong>Information Gain&lt;/strong>&lt;/li>
&lt;li>Gini Impurity for a leaf = 1 - (prob(&amp;ldquo;yes&amp;rdquo;) ^ 2 - prob(&amp;ldquo;no&amp;rdquo;) ^ 2)&lt;/li>
&lt;li>to work out toal gini impurity fpr the simple tree:&lt;/li>
&lt;li>= weigthed avg of gini impurities&lt;/li>
&lt;li>weigthed because a leaf may not represent the same number of people&lt;/li>
&lt;li>eg &amp;ldquo;loves popcorn? Yes? No?&amp;rdquo;&lt;/li>
&lt;li>For numeric data:&lt;/li>
&lt;li>sort by low to high&lt;/li>
&lt;li>calc avg value based of row below and current row&lt;/li>
&lt;li>then calc gini for each avg value&lt;/li>
&lt;li>this can be done e.g. avg val = 9.5&lt;/li>
&lt;li>root: &amp;ldquo;age &amp;lt; 9.5, loves song?&amp;rdquo;&lt;/li>
&lt;li>You then pick the root that represents num feature by the avg that = lowest gini&lt;/li>
&lt;li>You then pick the root by selecting the feature that has lowest gini!&lt;/li>
&lt;li>So split based off: &amp;ldquo;loves soda?&amp;rdquo;&lt;/li>
&lt;li>Can we reduce gini of branch by splitting people that loved soda or Age &amp;lt; ??? ?&lt;/li>
&lt;li>Calc gini for each of the two questions&lt;/li>
&lt;li>Select lowest gini of the two: &amp;ldquo;age &amp;lt; 12.5?&amp;rdquo;&lt;/li>
&lt;li>Can stop splitting based off if leaves are completly pure&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-prevent-overfitting">How to Prevent Overfitting&lt;/h2>
&lt;ul>
&lt;li>Let say that leaf only has one person, can be hard to be sure that it does a good job of making preds&lt;/li>
&lt;li>could overfit the data&lt;/li>
&lt;li>Can deal with this by:&lt;/li>
&lt;li>pruning&lt;/li>
&lt;li>can put limits on hwo trees grow by requiring 3 or more people per leaf&lt;/li>
&lt;/ul>
&lt;h1 id="regression-trees">Regression Trees&lt;/h1>
&lt;ul>
&lt;li>use when you cant fit straight line to data&lt;/li>
&lt;li>to work out output of leaf, just get avg of the values that sit in the leaf&lt;/li>
&lt;li>building root:&lt;/li>
&lt;li>dosage &amp;lt; 3 &amp;ndash; 3 is avg of two values&lt;/li>
&lt;li>get avg of vals of &amp;lt; 3 and &amp;gt;= 3 (0 &amp;amp; 38.8)&lt;/li>
&lt;li>so tree will say: &amp;ldquo;Dosage &amp;lt; 3? If so, then 0, else 38.8.&amp;rdquo;&lt;/li>
&lt;li>use sum of square residuals of all points (left and right) in the leaves to get qual of preds&lt;/li>
&lt;li>THEN, go to next two points e.g. &amp;ldquo;dosage &amp;lt; 5&amp;rdquo;&lt;/li>
&lt;li>Select smallest sum of squared residuals&lt;/li>
&lt;li>root = &amp;ldquo;dosage &amp;lt; 14.5?&amp;rdquo;&lt;/li>
&lt;li>You then for the left and right branch can do the same again on the samller subset of values&lt;/li>
&lt;li>you then need to do this for the other features in data&lt;/li>
&lt;li>compare SSR for each feature that makes the root node&lt;/li>
&lt;li>to the branch down, you just do the same as before but it&amp;rsquo;s filtered on e.g. &amp;ldquo;Age &amp;gt; 50&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h1 id="how-to-prune-regression-trees">How to Prune Regression Trees&lt;/h1>
&lt;ul>
&lt;li>there are several methods of prunin&lt;/li>
&lt;li>this is on &lt;strong>Cost Complexity Pruning&lt;/strong> or &lt;strong>Weakest Link Pruning&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>use this for overfitting and variance
let say we have test/train data and the model is overfit to test data&lt;/p>
&lt;ul>
&lt;li>one way to reduce overfitting is by reducing the number of leaves&lt;/li>
&lt;li>replace the split with a leaf that is avg of a larger num of observations&lt;/li>
&lt;li>e.g. remove two leaves of branch and replace branch by leaf&lt;/li>
&lt;li>now the sub-tree does better at predicting test data but worse at train&lt;/li>
&lt;/ul>
&lt;p>how do we decide which sub-tree to use???&lt;/p>
&lt;ul>
&lt;li>
&lt;p>first tep of cost complex pruning is to calc SSR of each pruned sub-tree for trian data&lt;/p>
&lt;/li>
&lt;li>
&lt;p>calc SSR for each leaf&lt;/p>
&lt;/li>
&lt;li>
&lt;p>then add up all SSR for all leaves&lt;/p>
&lt;/li>
&lt;li>
&lt;p>each time a branch/leaf is removed, the SSR for the sub-tree will get bigger&lt;/p>
&lt;/li>
&lt;li>
&lt;p>not a suprise tho ofc&lt;/p>
&lt;/li>
&lt;li>
&lt;p>how comapre trees?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CCP based on:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>tree score = SSR + alpha (tree complexity pen, find using cross validation) * T (num of leaves or terminal nodes)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>do this for all sub-trees&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pick lowest tree score sub-tree&lt;/p>
&lt;/li>
&lt;li>
&lt;p>how to build pruned regression tree?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>first use ALL data build full sized reg tree&lt;/p>
&lt;/li>
&lt;li>
&lt;p>now increase alpha till the sub-tree with one branch removed, gives lower tree score compared to full sized tree&lt;/p>
&lt;/li>
&lt;li>
&lt;p>keep doing this with smaller and smaller sub-trees&lt;/p>
&lt;/li>
&lt;li>
&lt;p>recording alpha values for each&lt;/p>
&lt;/li>
&lt;li>
&lt;p>in the end, different vals for alpha give use a sequence of tree from full sized to just a leaf&lt;/p>
&lt;/li>
&lt;li>
&lt;p>e.g. 0, 10k, 15k, 22k&lt;/p>
&lt;/li>
&lt;li>
&lt;p>now divide into train and test&lt;/p>
&lt;/li>
&lt;li>
&lt;p>using just train data, use the alpha values we found on full tree and a sequaence of subtrees that min the tree score&lt;/p>
&lt;/li>
&lt;li>
&lt;p>now calc SSR of each new tree for test data and pick alpha based off lowest SSR&lt;/p>
&lt;/li>
&lt;li>
&lt;p>repeat using cross-validation&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pick alpha that on avg gave lowest SSR with the testing data in each cross fold&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="random-forests">Random Forests&lt;/h1>
&lt;p>Step 1: Create a &amp;ldquo;&lt;strong>bootstrapped&lt;/strong>&amp;rdquo; dataset&lt;/p>
&lt;ul>
&lt;li>Randomly sample dataset so that we have same size as orginal dataset&lt;/li>
&lt;li>can pick same sample more than once&lt;/li>
&lt;/ul>
&lt;p>Step 2: Create decision tree using boostrapped dataset but only use a random subset of features at each step&lt;/p>
&lt;ul>
&lt;li>
&lt;p>e.g. only consider 2 cols at each step&lt;/p>
&lt;/li>
&lt;li>
&lt;p>there can be an optimal num of cols&lt;/p>
&lt;/li>
&lt;li>
&lt;p>once root has been determined out of 2 cols, remove that col from data to focus on ALL of the remaining cols&lt;/p>
&lt;/li>
&lt;li>
&lt;p>for next branch, randomly select 2 cols of the remaining cols&lt;/p>
&lt;/li>
&lt;li>
&lt;p>in the next levels of branches, you can select the col that was removed for the parent node&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Now go back to step 1 and repeat bootstrap and build tree again&lt;/p>
&lt;p>How to use forest?&lt;/p>
&lt;ul>
&lt;li>all trees vote and pick answer based off all the trees, 5 Trues and 1 False = True classification&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>B&lt;/strong>oostrapping the data plus the &lt;strong>agg&lt;/strong>regate to make a decision is called &lt;strong>Bagging&lt;/strong>&lt;/p>
&lt;p>We can have &lt;strong>Out-Of-Bag-Dataset&lt;/strong>, which are all the samples that did not make it into the bootstrapped data&lt;/p>
&lt;ul>
&lt;li>typically 1/3 of og data doesn&amp;rsquo;t make it&lt;/li>
&lt;li>run data through tree and check if it estimate correct&lt;/li>
&lt;li>do this out-of-bag sample through all of the other trees that were built without it&lt;/li>
&lt;li>do the same thing for all of the other out of bag samples for all of the trees&lt;/li>
&lt;/ul>
&lt;p>The proportion of out-of-bag samples that were incorrectly calssifies is the &lt;strong>Out-Of-Bag Error&lt;/strong>&lt;/p>
&lt;p>To tune the number of cols selected for each step, use &lt;strong>Out-of-bag error&lt;/strong>&lt;/p>
&lt;h1 id="adaboost">Adaboost&lt;/h1>
&lt;p>Trees are usually just a root and two leaves&lt;/p>
&lt;ul>
&lt;li>This is called a stump&lt;/li>
&lt;li>They are not great at making accurate classifications&lt;/li>
&lt;li>&amp;ldquo;&lt;em>weak learners&lt;/em>&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>In a forest of stumps, some stumps get more say in final classification than others&lt;/p>
&lt;ul>
&lt;li>this is in contrast with RF&lt;/li>
&lt;/ul>
&lt;p>Order is important&lt;/p>
&lt;ul>
&lt;li>The error that the first stump makes, influences how the second stump is made, etc etc&lt;/li>
&lt;li>In RF the trees are independently generated&lt;/li>
&lt;/ul>
&lt;p>First give each row in dataset a sample weight&lt;/p>
&lt;ul>
&lt;li>To begin with, this will be 1 / N&lt;/li>
&lt;/ul>
&lt;p>Then create root node in the same way as a decision tree&lt;/p>
&lt;p>The the &lt;strong>total error&lt;/strong> will determine the &lt;strong>amount of say&lt;/strong> a stump has:&lt;/p>
&lt;ul>
&lt;li>amount of say = 0.5 * log((1 - total error) / total error)&lt;/li>
&lt;li>if total error = 0.5, say = 0&lt;/li>
&lt;li>if total error apporaches -1 (you would just do opposite of what tree says) or 1, say increases&lt;/li>
&lt;li>if total error = 1, the equation freaks out (log(0)), so a small error term is added to correct for this&lt;/li>
&lt;/ul>
&lt;p>To update the rows that the stump incorrectly guesses:&lt;/p>
&lt;ul>
&lt;li>new sample weight = sample weight * e ^ (amount of say)&lt;/li>
&lt;li>So if stump has a lot of say and gets guess wrong, sample weight will drastically increase&lt;/li>
&lt;/ul>
&lt;p>If the stump gets the guess correct, update the sample weight:&lt;/p>
&lt;ul>
&lt;li>new sample weight = sample weight * e ^ -(amount of say)&lt;/li>
&lt;li>if amount of say is large, then the row weight will go down a lot&lt;/li>
&lt;/ul>
&lt;p>After all weights have been updated, you need to nomalise the weights so that they add up to 1&lt;/p>
&lt;ul>
&lt;li>do this by dividing each weight by the sum of the weights&lt;/li>
&lt;/ul>
&lt;p>In theory, the new sample weights could be used to calc a weighted gini index for a new tree&lt;/p>
&lt;p>In practice, you generate rng nums between 0 and 1, selecting the row that it falls on base don the cum sample weights&lt;/p>
&lt;p>Do this as many times as you want!&lt;/p>
&lt;p>How do the trees make predicition?&lt;/p>
&lt;ul>
&lt;li>Add up the amount of say of each True and False preds&lt;/li>
&lt;li>The one with more total say will be the pred&lt;/li>
&lt;/ul>
&lt;h1 id="least-squares-linear-regression">Least Squares Linear Regression&lt;/h1>
&lt;ol>
&lt;li>Draw line through data&lt;/li>
&lt;li>Sum pred - obvs for all points&lt;/li>
&lt;li>Square each term in sum to negatate large obvs vs pred&lt;/li>
&lt;li>This is &lt;strong>sum of squared residuals&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>$y = ax + b$
We want to find the optimal vaues for a and b so that the sum of squared residuals is minimised
Since we want the line that will give us the smallest sum of squares, this is called &lt;em>&lt;strong>least squares&lt;/strong>&lt;/em>&lt;/p>
&lt;p>Plot graph of roration of line vs SSR and take derivative of the function
Best point is where the derivative is 0&lt;/p>
&lt;h1 id="linear-regression">Linear Regression&lt;/h1>
&lt;p>Main ideas:&lt;/p>
&lt;ol>
&lt;li>Use least-squares to fit a line to the data&lt;/li>
&lt;li>Calculate $R^2$&lt;/li>
&lt;li>Calculate a p-value for $R^2$&lt;/li>
&lt;/ol>
&lt;p>Look above for notes on least-squares&lt;/p>
&lt;p>$R^2$:
Example:&lt;/p>
&lt;ul>
&lt;li>First shift all data to y-axis&lt;/li>
&lt;li>We are only interested in y-axis feature&lt;/li>
&lt;li>Then calculate avg. y data&lt;/li>
&lt;li>Sum the squared resdiduals&lt;/li>
&lt;li>We call this SS(mean) -&amp;gt; &amp;ldquo;sum of squares around the mean&amp;rdquo;&lt;/li>
&lt;li>SS(mean) = (data - mean)^2&lt;/li>
&lt;li>Variation around the mean = (data - mean) ^ 2 / n&lt;/li>
&lt;li>Var(mean) = SS(mean) / n&lt;/li>
&lt;li>We can think of variance as the average sum of squares per e.g. &amp;ldquo;mouse&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>Now apply this to the line of best fit:&lt;/p>
&lt;ul>
&lt;li>SS(fit) = (data - line) ^ 2&lt;/li>
&lt;li>Var(fit) = SS(fit) / n&lt;/li>
&lt;li>Can think of Var(fit) as the avg SS(fit) for each mouse&lt;/li>
&lt;/ul>
&lt;p>In general: Variance(something) = sum of squares / the number of those things = average sum of squares&lt;/p>
&lt;p>There is less variation around the line that we fit by least-squares&lt;/p>
&lt;ul>
&lt;li>We can say that some for the variation in mouse size is &amp;ldquo;explained&amp;rdquo; by taking mouse weight into account&lt;/li>
&lt;/ul>
&lt;p>&lt;em>&lt;strong>So $R^2$ tells up how much of the variation in mouse size can be explained by taking mouse weight into account.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>$R^2 = \frac{Var(mean) - Var(fit)}{Var(mean)}$&lt;/p>
&lt;p>Example: &lt;em>$R^2$ = 0.6 = 60%: &amp;ldquo;There is a 60% reduction in variance when we take the mouse weight into account&amp;rdquo;&lt;/em> or &lt;em>&amp;ldquo;mouse weight explains 60% of the variation in mouse size&amp;rdquo;&lt;/em>.&lt;/p>
&lt;p>You can also use the sums of squares to make the same calc:
$R^2 = \frac{SS(mean) - SS(fit)}{SS(mean)}$&lt;/p>
&lt;p>Adding terms will never reduce $R^2$!
This is because lesat-sqaures would make the offending term(s) go to 0&lt;/p>
&lt;ul>
&lt;li>Cause any term that makes SS(fit) worse to be multiplied by 0&lt;/li>
&lt;li>However, the more stupid paramters added to the equation, the more chance there for random events to result in a better $R^2$&lt;/li>
&lt;li>Because of this, people report an &amp;ldquo;adjusted&amp;rdquo; $R^2$ value that in essence, scales R^2 by the number of parameters&lt;/li>
&lt;/ul>
&lt;p>What if we had only 2 measurements however?&lt;/p>
&lt;ul>
&lt;li>SS(fit) = 0 =&amp;gt; R^2 = 100%&lt;/li>
&lt;li>Any two random points would give exact same thing, doesnt mean anything&lt;/li>
&lt;li>So need a stats test to prove it&amp;rsquo;s significant&lt;/li>
&lt;/ul>
&lt;p>The p-value for R^2 comes from something called F:&lt;/p>
&lt;ul>
&lt;li>Tells you how reliable the fit is!&lt;/li>
&lt;li>F = the variation explained by the extra parameters in the fit / the variation not explained by the extra parameters in the fit&lt;/li>
&lt;li>$F = \frac{SS(mean) - SS(fit) / (p_{fit} - p_{mean})}{SS(fit) / (n - p_{fit})}$&lt;/li>
&lt;li>p are the degrees of freedom&lt;/li>
&lt;li>Why divide SS(fit) by $n - p_{fit}$ instead of just n?&lt;/li>
&lt;li>The more params you have in your equation, the more data you need to estimate them&lt;/li>
&lt;li>For example, you only need two points to estimate a line, but you need 3 points to estimate a plane&lt;/li>
&lt;li>If the fit is good F = large num / small num&lt;/li>
&lt;/ul>
&lt;p>To calc p-value:&lt;/p>
&lt;ul>
&lt;li>You could gen histogram from lots of random datasets and then see where you f-score for the dataset sits&lt;/li>
&lt;li>What is more common is to generate an F-dstribution based off a line: $(p_{mean} - p_{fit}) = 1$, $(n - p_{fit}) = 10$&lt;/li>
&lt;li>the number of degrees of freedom determines the shape of the line&lt;/li>
&lt;/ul>
&lt;h1 id="maximum-likelihood">Maximum Likelihood&lt;/h1>
&lt;p>The goal of maximum likelihood is the find the optimal way to fit a distribution to the data.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;ul>
&lt;li>Take normal distribution&lt;/li>
&lt;li>fit a bell curve with centre far from avg of points&lt;/li>
&lt;li>most of the points have a low probability or &amp;ldquo;likelihood&amp;rdquo; of being observed in the places they are&lt;/li>
&lt;li>if you shift the curve over so that its mean was the same as the average?&lt;/li>
&lt;li>the likelihood of observing the points would be high&lt;/li>
&lt;li>if you kept moving it over, the likelihood of observing the points would be low again&lt;/li>
&lt;li>with the best mean found, you need to do the same for std. dev.&lt;/li>
&lt;/ul>
&lt;p>probability vs likelihood:&lt;/p>
&lt;ul>
&lt;li>likelihood specifically refers to this situation here; you are trying to find the optimal value for the mean or std dev for a dist given a bunch of observed measurements&lt;/li>
&lt;/ul>
&lt;h1 id="odds-and-logodds">Odds and Log(Odds)&lt;/h1>
&lt;p>&amp;ldquo;The odds in favour of my team winning the game are 1 to 4&amp;rdquo;&lt;/p>
&lt;p>odds are not probability&lt;/p>
&lt;ul>
&lt;li>they are the ratio of something happening vs to something not happening&lt;/li>
&lt;li>prob is the ratio of something happening vs everything happening&lt;/li>
&lt;/ul>
&lt;p>prob of something / prob of not something is the same as the odds&lt;/p>
&lt;ul>
&lt;li>odds of team losing go from 0 to 1 (1 : 32 =&amp;gt; 0.03125)&lt;/li>
&lt;li>odds of team winning go from 1 to $\infty$ (32 : 3 =&amp;gt; 10.67)&lt;/li>
&lt;li>this makes it difficult to compare the odds of winning vs losing&lt;/li>
&lt;li>taking the log of the odds helps with this&lt;/li>
&lt;/ul>
&lt;p>the log of the ratio of probs is called the &lt;em>&lt;strong>logit function&lt;/strong>&lt;/em> and formthe basis for logistic regression&lt;/p>
&lt;p>log(odds) are very useful for solving certain stats problems&lt;/p>
&lt;ul>
&lt;li>specifically ones where we are trying to determin probs about win/lose, yes/no or true/false questions&lt;/li>
&lt;li>plotting dist of win : lose of many things will result in normal dist&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Has Cancer/Has mutated gene&lt;/th>
&lt;th>Yes&lt;/th>
&lt;th>No&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Yes&lt;/td>
&lt;td>23&lt;/td>
&lt;td>117&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>No&lt;/td>
&lt;td>6&lt;/td>
&lt;td>210&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Odds ratio: $\frac{23 / 117}{6 / 210} = \frac{0.2}{0.03} = 6.88$
This means that the odds are 6.88 times greater that someone with the mutated gene will also have cancer
Hence log ratio: $1.93$&lt;/p>
&lt;ul>
&lt;li>The odds ratio and log of the odds ratio are like R-squared; they indicate a relationship between two things&lt;/li>
&lt;li>Larger values mean that the mutated gene is a good predictor of cancer&lt;/li>
&lt;li>Values close to 0 are bad: $log(1) = 0$&lt;/li>
&lt;li>$log(0.001) = -6.91$ (so do opposite) (for every game I win someone else wins 1,000 times)&lt;/li>
&lt;li>$log(100) = 4.6$ (for every game I lose, I win 100)&lt;/li>
&lt;li>hence these are good predictors&lt;/li>
&lt;/ul>
&lt;p>3 ways to determine if an odds ratio is statistically signficant:&lt;/p>
&lt;ul>
&lt;li>Fisher&amp;rsquo;s Exact Test&lt;/li>
&lt;li>Chi-Square Test&lt;/li>
&lt;li>The Wald test
No consensus on what is best, so people just mix and match:&lt;/li>
&lt;li>Some people use Fisher&amp;rsquo;s/Chi-Square test to get p-values&lt;/li>
&lt;li>Wald Test to calculate a confidence interval&lt;/li>
&lt;li>Others use just Wald Test for confidence interval/p-values&lt;/li>
&lt;li>So need to check what is best for your field&lt;/li>
&lt;/ul>
&lt;h1 id="logistic-regression">Logistic Regression&lt;/h1>
&lt;ul>
&lt;li>Predicts if something is True or False&lt;/li>
&lt;li>Fits an S shaped logistic function&lt;/li>
&lt;li>Test to see if a variable&amp;rsquo;s effect on the prediction is significantly different from 0&lt;/li>
&lt;li>If not, it means the variable isnot helping the prediction&lt;/li>
&lt;li>This is called Wald&amp;rsquo;s test&lt;/li>
&lt;li>Instead of using SSR and R^2 it uses &lt;em>&lt;strong>maximum likelihood&lt;/strong>&lt;/em>&lt;/li>
&lt;li>Logistic Regression is a specific type of &lt;strong>Generalised Linear Model&lt;/strong> (&lt;strong>GLM&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;p>Continuous values:
Because with logistic regression, the y-axis is confined to probability values between 0 and 1&lt;/p>
&lt;ul>
&lt;li>solve problem by transforming to the log odds, so like with linear regression the y-axis can go from $-\infty$ to $+\infty$&lt;/li>
&lt;/ul>
&lt;p>So we use the &lt;strong>logit function&lt;/strong>: $\log{\frac{p}{1 - p}}$&lt;/p>
&lt;ul>
&lt;li>So all values that are true (1) -&amp;gt; $\infty$&lt;/li>
&lt;li>All values that are false (0) -&amp;gt; $-\infty$&lt;/li>
&lt;/ul>
&lt;p>Take example logisitic regression line: $y = -3.48 + 1.83\times weight$
So the first coefficient is the y-axis intercept&lt;/p>
&lt;ul>
&lt;li>when weight (x-axis) = 0, then log(odds of obesity) = -3.476&lt;/li>
&lt;li>In other words, if you dont weigh anything, the odds are against you being obese!! Duh!&lt;/li>
&lt;li>The slope: for every one unit of weight gains, the log(odds of obesity) increases by 1.825&lt;/li>
&lt;/ul>
&lt;p>Descrete variable:&lt;/p>
&lt;p>&lt;em>&lt;strong>Need to come back to this (2nd vid for logistic regression)&lt;/strong>&lt;/em>&lt;/p>
&lt;h1 id="linear-models-for-t-tests-and-anova">Linear Models for T-Tests and ANOVA&lt;/h1>
&lt;p>&lt;em>&lt;strong>Need to come back to this&lt;/strong>&lt;/em>&lt;/p>
&lt;h1 id="gradient-descent">Gradient Descent&lt;/h1>
&lt;p>Can be used to optimise least squares like in notes above!
For: $y = ax + b$
start with least squares estimate for the slope a
we use gradient descent to find optimatal value for b&lt;/p>
&lt;p>set b = 0, any number will do
calc SSR&lt;/p>
&lt;ul>
&lt;li>NOTE: the SSR is a type of &lt;strong>loss function&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Keep changing the intercept and plot a graph of intercept vs SSR&lt;/p>
&lt;ul>
&lt;li>Inefficent way of doing this would be to calc lots and lots of points&lt;/li>
&lt;li>Gradient descent makes this way more efficent&lt;/li>
&lt;li>Does a few calcs when far from the optimal solution&lt;/li>
&lt;li>increases the number of calcs, close to the optimal value&lt;/li>
&lt;li>You can derive an equation for a line that describes the SSR&lt;/li>
&lt;li>Determine derivative of the function&lt;/li>
&lt;li>Then take steps towards min value approaches derivative = 0&lt;/li>
&lt;li>This is v useful because get around times where it&amp;rsquo;s impossible for derivative = 0&lt;/li>
&lt;li>The size of the step should be dependant on the size of the derivative&lt;/li>
&lt;li>This is determined by a &lt;strong>step size&lt;/strong> called the&lt;/li>
&lt;li>Mutiply the derivative by a small number called a &lt;strong>learning rate&lt;/strong>, the combination of these two is the step size&lt;/li>
&lt;li>So in this case, the new intercept = old intercept - step size&lt;/li>
&lt;li>This process repeats itself&lt;/li>
&lt;li>It will stop when the step size is very close to 0&lt;/li>
&lt;/ul>
&lt;p>How to do grad descent for both a and b?&lt;/p>
&lt;ul>
&lt;li>Take derivative with respect to the intercept also do the same with respect to the slope&lt;/li>
&lt;li>When you have 2 or more derivatives of the same function, they are called a &lt;strong>gradient&lt;/strong>&lt;/li>
&lt;li>This is why the algorithm is called &lt;strong>gradient descent&lt;/strong>!&lt;/li>
&lt;li>Pick rando number for intecept and slope&lt;/li>
&lt;li>Gives 2 SSRs&lt;/li>
&lt;li>Calc step size for both&lt;/li>
&lt;li>Adjust intercept and slope by step size&lt;/li>
&lt;/ul>
&lt;p>In general:&lt;/p>
&lt;ol>
&lt;li>Take the derivative of the &lt;strong>Loss Function&lt;/strong> for each parameter in it. Meaning take the &lt;strong>Gradient&lt;/strong> of the &lt;strong>Loss Function&lt;/strong>.&lt;/li>
&lt;li>Pick rando values for all parameters.&lt;/li>
&lt;li>Plug param values into the gradient&lt;/li>
&lt;li>Calc step size&lt;/li>
&lt;li>calculate new params&lt;/li>
&lt;li>Go back to step 3 and repeat till step size small or max num of steps reached&lt;/li>
&lt;/ol>
&lt;p>For a lot of data points, this can take long time
So, &lt;strong>stochastic gradient descent&lt;/strong> that uses a rando selected subset of the data at every step, rather than the full dataset&lt;/p>
&lt;h1 id="gradient-boosting-regression-main-ideas">Gradient Boosting: Regression Main Ideas&lt;/h1>
&lt;p>Grad boost very similar to AdaBoost&lt;/p>
&lt;p>When starting, grad boost creates a single leaf that is the avg of the target var
Then grad bost builds a tree, that is based on the errors of the previous tree.&lt;/p>
&lt;ul>
&lt;li>This tree is larger than stump but still restricts the size of the tree&lt;/li>
&lt;li>In practice, people ofen set the num of leaves to be between 8 and 32&lt;/li>
&lt;/ul>
&lt;p>Grad boost will scale each tree by a fixed amount, unlike AdaBoost (amount of say)
It will then build tree based of the errors of the previous tree.
It will then build trees in this fashon till made num of trees asked for or aditional trees fail to improve fit.&lt;/p>
&lt;ol>
&lt;li>Calc avg of the target feature as first &amp;rsquo;tree&amp;rsquo;/leaf&lt;/li>
&lt;li>Better way: $F_{0}(x) = \arg \max_{\gamma}\sum_{i=1}^{n}L(y_{i}, \gamma)$, $L$ is loss func&lt;/li>
&lt;li>Either find best value via grad descent or if ez derivative to solve for 0, do that&lt;/li>
&lt;li>Build tree on errors of first tree: error = obvs - pred, this is called a &lt;strong>pseudo residual&lt;/strong>. The pseudo part is a reminder that we are doing grad boost, not linear reg.&lt;/li>
&lt;li>Now use input vars to predict the residuals to build tree. A leaf of this tree may pointwards two residuals. If this happens, get avg of residuals on leaf.&lt;/li>
&lt;li>Now combine original leaf with the new tree for pred. Effectively this new tree &amp;lsquo;corrects&amp;rsquo; for the error of the og leaf. After doing this once, the bias will be low but varience high (overfit), so there is a learning rate to correct for this. Multiple learning rate by new tree&amp;rsquo;s residual pred. Between 0 and 1. This results in a small step in the right direction. Taking lots of small steps in the right direction results in lower varience.&lt;/li>
&lt;li>Calc psuedo residuals again based of new preds and do the same.&lt;/li>
&lt;li>Keep doing this until num trees reached or the sum of residuals do not significantly decrease.&lt;/li>
&lt;/ol>
&lt;p>Gradient boost is called gradient boost because the residual is the &lt;strong>Gradient&lt;/strong> that comes from &lt;strong>Gradient descent&lt;/strong>.&lt;/p>
&lt;h1 id="gradient-boost-classification-main-ideas">Gradient Boost: Classification Main Ideas&lt;/h1>
&lt;p>&lt;em>&lt;strong>Need to come back to this!&lt;/strong>&lt;/em>&lt;/p>
&lt;h1 id="regularization-ridge-l2-regression">Regularization: Ridge (L2) Regression&lt;/h1>
&lt;ul>
&lt;li>Another way of saying desensitisation (lol)&lt;/li>
&lt;/ul>
&lt;p>Use Linear regression aka least squares:&lt;/p>
&lt;ul>
&lt;li>example: has intercept and slope&lt;/li>
&lt;li>training data has 2 points&lt;/li>
&lt;li>testing has 8&lt;/li>
&lt;li>SSR = 0 for training, for testing is high&lt;/li>
&lt;li>high varience (overfit to training)&lt;/li>
&lt;li>Main idea behind ridge regression is to find a new line that doesn&amp;rsquo;t fit the training data as well&lt;/li>
&lt;li>introduce a small amount of bias&lt;/li>
&lt;li>in return we get a significant drop in variance&lt;/li>
&lt;/ul>
&lt;p>Example: size = intercept + slope * Weight
Lin reg minises: SSR
Ridge regression tries to minimises: SSR + $\lambda \times {slope}^2$&lt;/p>
&lt;ul>
&lt;li>The added tern adds a penalty to the traditional least squares method&lt;/li>
&lt;/ul>
&lt;p>So when the slope of the line is steep, then the prediction for size is very sensitive to relatively small changes in weight
So predictions made with the ridge regression line are less sensistive to weight than the least squares line
The large lambda, the slope get asymptotically close to 0
To find the value for lambda, try typically 10-fold cross validation to determine which one results in the lowest variance&lt;/p>
&lt;p>Also works on descrete data:
size = 1.5 + 0.7 * high fat diet&lt;/p>
&lt;p>so ridge regression minimises: SSR + $\lambda \times {diet difference}^2$&lt;/p>
&lt;ul>
&lt;li>diet difference refers to the distance in size between normal diet and high far diet&lt;/li>
&lt;/ul>
&lt;p>for more complex model:
size = intercept + slope * weight + diet difference * high fat diet
so penalty term is: lambda * (slope ^ 2 + diet difference ^ 2)&lt;/p>
&lt;p>so in general, the penalty term will apply itself to every parameter, except for the intercept, is scaled by the measurements&lt;/p>
&lt;p>you cant make a line with one data point
nor a plane with 2 data points&lt;/p>
&lt;ul>
&lt;li>you would need at least 10,001 data points to make an equation of 10,001 parameters,&lt;/li>
&lt;li>sometimes that&amp;rsquo;s not possible&lt;/li>
&lt;li>what do you do here?&lt;/li>
&lt;/ul>
&lt;p>use ridge regression!&lt;/p>
&lt;ul>
&lt;li>The penalty can solve for all 10,001 params with 500 samples or fewer&lt;/li>
&lt;/ul>
&lt;h1 id="regularization-lasso-l1-regression">Regularization: Lasso (L1) Regression&lt;/h1>
&lt;p>Very similar to ridge regression but has some important differences
Intead the penalty is: $\lambda \times |slope|$&lt;/p>
&lt;ul>
&lt;li>adds a bit of bias but less variarience than leat squares&lt;/li>
&lt;/ul>
&lt;p>The big difference is that L1 can make the slope go to 0&lt;/p>
&lt;ul>
&lt;li>Ridge can only do this asymptotically&lt;/li>
&lt;/ul>
&lt;p>This can eleminate any terms in an equation that are stupid to 0&lt;/p>
&lt;p>So lasso does better when there are lots of useless parameters
Ridge does better then most of the variables are useful&lt;/p>
&lt;h1 id="xgboost-regression">XGBoost: Regression&lt;/h1>
&lt;p>Has lots of parts but are simple:&lt;/p>
&lt;ol>
&lt;li>Gradient boost(ish) (wont be covered as done before)&lt;/li>
&lt;li>Regularization (wont be covered as done before)&lt;/li>
&lt;li>A unique regression tree&lt;/li>
&lt;li>Approximate greedy algorithm&lt;/li>
&lt;li>Weighted quantile sketch&lt;/li>
&lt;li>Sparsity-aware split finding&lt;/li>
&lt;li>Parallel learning&lt;/li>
&lt;li>Cache-aware access&lt;/li>
&lt;li>Blocks for out-of-core computation&lt;/li>
&lt;/ol>
&lt;p>XGBoost was designed to be used with large, complicated datasets&lt;/p>
&lt;h2 id="unique-regression-tree">Unique Regression Tree&lt;/h2>
&lt;p>First step in fitting XGBoost to the training data is to make an initial prediction&lt;/p>
&lt;ul>
&lt;li>This prediction can be anything but by default it is 0.5, regardless if it&amp;rsquo;s regression or classification&lt;/li>
&lt;/ul>
&lt;p>Like with gradient boost, XGBoost fits a regression tree to the residuals&lt;/p>
&lt;ul>
&lt;li>However, XGBoost uses a unique regression tree that the vid calls XGBoost tree
There are many ways this tree can be built, but this is the most common way for regression&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Each tree starts with a single leaf, all residuals go to the leaf&lt;/li>
&lt;li>Calculate a quality score or similarity score for the residuals&lt;/li>
&lt;li>Similarity score = SSR / (num residuals + $\lambda$)&lt;/li>
&lt;li>This similarity score is a simplification of second order Taylor approximation of the loss function described in grad boost&lt;/li>
&lt;li>Also, the lambda here will just be for L2 regularisation&lt;/li>
&lt;li>L1 will make the sim score simplify to something different&lt;/li>
&lt;li>Now the question is whether or not we can do a better job clustering similar residuals if we split them into two groups&lt;/li>
&lt;li>Focus on the two points that are smallest (x-axis)&lt;/li>
&lt;li>Get avg value&lt;/li>
&lt;li>Make a split based off the avg value&lt;/li>
&lt;li>Calc similarity score for both leafs&lt;/li>
&lt;li>Now need to quanitify how much better the leaves cluster similar resdiuals than the root, do this by calculating the gain&lt;/li>
&lt;li>$gain = left_{similarity} + right_{similarity} - root_{similarity}$&lt;/li>
&lt;li>Now shift the threshold over so that it is the avg of th next two observation&lt;/li>
&lt;li>More gain is good, is better at splitting the residuals into clusters of similar values&lt;/li>
&lt;li>Get best tree based of thresholds&lt;/li>
&lt;li>Try to split leaves further, the root in the gain calc refers to the parent node of the leaves&lt;/li>
&lt;li>Keep going till at max depth (default is 6)&lt;/li>
&lt;/ol>
&lt;p>How to prune this tree?&lt;/p>
&lt;ul>
&lt;li>Pruned based on its gain values&lt;/li>
&lt;li>Start with a number, for example, 130&lt;/li>
&lt;li>This is called $\gamma$ gamma&lt;/li>
&lt;li>Then calc the difference between the gain asscociated with the lowest branch in the tree and gamma: $gain - \gamma$&lt;/li>
&lt;li>if the difference is positive then the branch is not removed, if it is neg then remove&lt;/li>
&lt;li>once you find a pos difference, then stop pruning&lt;/li>
&lt;li>This can also remove the root of the tree, leaving just the original prediction, which is pretty extreme pruning&lt;/li>
&lt;/ul>
&lt;p>The regularization term:&lt;/p>
&lt;ul>
&lt;li>This will decrease similarity scores&lt;/li>
&lt;li>This will tend to stop the trees getting so large as it&amp;rsquo;s harder for a branch to have a lower sim score than its parent&lt;/li>
&lt;li>This also can make the pruning more agressive as $\gamma$ is more likely to be larger than the gain&lt;/li>
&lt;li>In the case of gain being less than 0, setting gamma = 0 will not turn off pruning&lt;/li>
&lt;/ul>
&lt;p>Output value = sum of residuals / (num of residuals + $\lambda$)&lt;/p>
&lt;ul>
&lt;li>This refers to the output value for each leaf&lt;/li>
&lt;li>Pretty similar to the similarity score except it&amp;rsquo;s not SSR&lt;/li>
&lt;li>This also means that lambda will reduce the amount a leaf will contribute to a prediction&lt;/li>
&lt;li>So will reduce the sensistivity to an indivual observation&lt;/li>
&lt;/ul>
&lt;p>Like with gradient boost, there is a learning rate that scales the tree&lt;/p>
&lt;ul>
&lt;li>XGboost calls this learning rate: $\epsilon$ (eta but this is epsilon?)&lt;/li>
&lt;li>Default value is 0.3&lt;/li>
&lt;/ul>
&lt;p>So output = inital pred + learning rate * output value for the leaf that the point goes down to&lt;/p>
&lt;p>Now build new tree based on the new residuals&lt;/p>
&lt;p>The two reasons to use XGBoost are also the two goals of the project:&lt;/p>
&lt;ul>
&lt;li>Execution Speed&lt;/li>
&lt;li>Model Performance&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Sparse Aware&lt;/strong>: implementation with automatic handling of missing data values.
&lt;strong>Block Structure&lt;/strong>: to support the parallelization of tree construction.
&lt;strong>Continued Training&lt;/strong>: so that you can further boost an already fitted model on new data.&lt;/p>
&lt;h2 id="approximate-greedy-algorithm">Approximate Greedy Algorithm&lt;/h2>
&lt;p>When fitting a tree to the residuals&lt;/p>
&lt;ul>
&lt;li>was done by calculating the similarity scores and the gain for each possible threshold&lt;/li>
&lt;li>the threshold with the largest gain is the one XGBoost uses&lt;/li>
&lt;li>The decision to use the threshold that gives the largest gain is made without worrying about how the leaves will be split later&lt;/li>
&lt;li>This means XGBoost uses a greedy algorithm&lt;/li>
&lt;li>In other words, since XGBoost uses a greedy algorithm, it amkes a decision without looking ahead to see if it is the absolute best choice in the long term&lt;/li>
&lt;li>If it did not use this algo, it woukld postpone making a final decision about this threhold, until after trying different thresholds in the leaves to see how things played out in the long run&lt;/li>
&lt;li>So, the greedy algo makes XGBoost a tree relatively quickly&lt;/li>
&lt;/ul>
&lt;p>When you have lots of data points, the greedy algo still has to look through every possible threshold&lt;/p>
&lt;ul>
&lt;li>on top of that, you&amp;rsquo;d have to do this for every other feature in your data&lt;/li>
&lt;li>this would take forever&lt;/li>
&lt;li>this is where the approximate greedy algo comes in&lt;/li>
&lt;/ul>
&lt;p>Instead, divide the data into quantiles and use that for thresholds instead
By default, XGBoost uses &amp;ldquo;about&amp;rdquo; 33 quantiles&lt;/p>
&lt;ul>
&lt;li>The &amp;ldquo;about&amp;rdquo; comes from the Parallel Learning an Weighted Quantile Sketch&lt;/li>
&lt;/ul>
&lt;h2 id="parallel-learning--weighted-quantile-sketch">Parallel Learning &amp;amp; Weighted Quantile Sketch&lt;/h2>
&lt;p>Get your data and split it into small pieces and putting the peices on different computers on a network
The quantile sketch algorithm combines the alues form each computer to make an approximate histogram
Then the approximate histogram is used to calculate approximate quantiles
The Approximate greedy algorithm uses these approximate quantiles&lt;/p>
&lt;p>What about the weighted part?&lt;/p>
&lt;p>Usually, quantiles are set up so that the same number of observations are in each one&lt;/p>
&lt;ul>
&lt;li>In contrast, with weighted quantiles, each observation has a corresponding weight&lt;/li>
&lt;li>The sum of the weights are the same in each quantile&lt;/li>
&lt;li>The weights are derived from the cover metric&lt;/li>
&lt;li>the weight for each observation is the 2nd derivative of the loss function, what we are referring to as the Hessian (not Gradient)&lt;/li>
&lt;li>This means for regression the weights are all equal to 1&lt;/li>
&lt;li>This means that the the weighted quantiles are just like normal quantiles and contain an equal number of observations&lt;/li>
&lt;li>In constrast, for classification: weight = previous prob i * (1- previous prob i)&lt;/li>
&lt;/ul>
&lt;p>Only uses the approx greedy algo, parallel learning and the weighted quantile sketch then the training data is huge&lt;/p>
&lt;p>When the trainin set is not so large, a normal greedy algo is used&lt;/p>
&lt;h2 id="sparsity-aware-split-finding">Sparsity-Aware Split Finding&lt;/h2>
&lt;p>Have a few missing values&lt;/p>
&lt;ul>
&lt;li>Even with this, we can just use the base leaf to calculate the residuals for rows with missing data&lt;/li>
&lt;/ul>
&lt;p>split the data into two table&lt;/p>
&lt;ul>
&lt;li>one with all e.g. dosage values&lt;/li>
&lt;li>other with all values without dosage values&lt;/li>
&lt;/ul>
&lt;p>Table with dosage values:&lt;/p>
&lt;ul>
&lt;li>sort rows low to high&lt;/li>
&lt;li>calc candidate thresholds&lt;/li>
&lt;/ul>
&lt;p>The first gain value, gain left, is calculated by putting all of the resdiuals with missing dosage values into the leaf on the left, save gain
Do the same for leaf on the right, save gain&lt;/p>
&lt;p>Do this for all candidate thresholds
You then pick the tree with highest gain&lt;/p>
&lt;p>So example, Dosage &amp;lt; 15.5, going left will be the default path for all future observations that are missing dosage values&lt;/p>
&lt;h2 id="cache-aware-access">Cache-Aware Access&lt;/h2>
&lt;p>This is where XGBoost starts to get super nitty gritty&lt;/p>
&lt;p>Basic idea:&lt;/p>
&lt;ul>
&lt;li>Inside computer we have:&lt;/li>
&lt;li>CPU&lt;/li>
&lt;li>CPU has a small amount of cache memory &amp;ndash; CPU can use this mem faster than any other memory on the computer&lt;/li>
&lt;li>CPU attatched to large amount of main memory, larger but slower than cache&lt;/li>
&lt;li>HDD, very slow but largest&lt;/li>
&lt;/ul>
&lt;p>XGBoost puts the gradients and Hessians in the cache, so that is can rapdily calculate similarity scores and output scores&lt;/p>
&lt;h2 id="blocks-for-out-of-core-computation">Blocks for Out-of-Core Computation&lt;/h2>
&lt;p>When the dataset is too large for the cache and main memory, then some of it must be stored on the HDD&lt;/p>
&lt;p>Because reading and writing data to the HDD is super slow, XGBoost tries minimising these actions by compressing the data&lt;/p>
&lt;p>When there is more than one HDD, XGBoost uses a databse technique called sharding to speed up disk access&lt;/p>
&lt;ul>
&lt;li>The when the CPU needs data, both drives can be reading data at the same time&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>Finally, XGHBoost can speed things up by allowing you to build each tree with only a random subset of the data.&lt;/p>
&lt;p>AND, can build trees by only looking at a random subset of features when deciding how to split the data.&lt;/p></content></item><item><title>Bulgarian Counting To 10</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-counting-to-10/</link><pubDate>Wed, 15 Feb 2023 18:29:40 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-counting-to-10/</guid><description> Bulgarian 0 нула 1 един (m), една (f), едно (n) 2 два (m), две (n/f) 3 три 4 четири 5 пет 6 шест 7 седем 8 осем 9 девет 10 десет References</description><content>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Bulgarian&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>нула&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>един (m), една (f), едно (n)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>два (m), две (n/f)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>три&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>четири&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>пет&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>шест&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>седем&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>осем&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>девет&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>десет&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Numeral Plural</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/numeral-plural/</link><pubDate>Wed, 15 Feb 2023 18:29:40 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/numeral-plural/</guid><description>Some masculine words have specials rules, taking a different plural ending. This happens when these words follow a number or expressions such as колко (how many?) or няколко (a few). In these cases, they will end with -a.
&amp;ldquo;един месец&amp;rdquo; - one month
&amp;ldquo;два месеца&amp;rdquo; - two months
&amp;ldquo;един ден&amp;rdquo; - one day
&amp;ldquo;няколко дена*&amp;rdquo; - a few days
This is called the numeral plural or counting form (бройна форма). This only applies to masculine nouns that do not represent people.</description><content>&lt;p>Some masculine words have specials rules, taking a different plural ending. This happens when these words follow a number or expressions such as колко (how many?) or няколко (a few). In these cases, they will end with -a.&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;един месец&amp;rdquo; - one month&lt;/p>
&lt;p>&amp;ldquo;два месеца&amp;rdquo; - two months&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&amp;ldquo;един ден&amp;rdquo; - one day&lt;/p>
&lt;p>&amp;ldquo;няколко дена*&amp;rdquo; - a few days&lt;/p>
&lt;/blockquote>
&lt;p>This is called the &lt;em>numeral plural&lt;/em> or &lt;em>counting form&lt;/em> (бройна форма). This only applies to masculine nouns that do not represent people.&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>Bulgarians often use &amp;ldquo;дни&amp;rdquo;, the normal plural, instead of &amp;ldquo;дена&amp;rdquo;, the numeral plural. Therefore we&amp;rsquo;ll consider both &amp;ldquo;два дена&amp;rdquo; and &amp;ldquo;два дни&amp;rdquo; as correct. &amp;ldquo;Дни&amp;rdquo; is much more commonly heard, despite &amp;ldquo;дена&amp;rdquo; being the grammatically correct version.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Closed Questions</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-closed-questions/</link><pubDate>Wed, 15 Feb 2023 18:28:05 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-closed-questions/</guid><description>Asking a closed question (one that can be answered with yes or no) is quite simple.
Use the ли particle, usually added after the verb With the verb to be (съм) in the present tense, it is placed before Examples:
Има ли котка? - Is there a cat? Имаш ли куче? - Do you have a dog? Момче ли си? - Are you a boy? References</description><content>&lt;p>Asking a closed question (one that can be answered with yes or no) is quite simple.&lt;/p>
&lt;ul>
&lt;li>Use the ли particle, usually added after the verb&lt;/li>
&lt;li>With the verb to be (съм) in the present tense, it is placed before&lt;/li>
&lt;/ul>
&lt;p>Examples:&lt;/p>
&lt;ul>
&lt;li>Има ли котка? - Is there a cat?&lt;/li>
&lt;li>Имаш ли куче? - Do you have a dog?&lt;/li>
&lt;li>Момче ли си? - Are you a boy?&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian There Is</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-there-is/</link><pubDate>Wed, 15 Feb 2023 18:26:36 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-there-is/</guid><description>The 3rd person singular of имам is also used to mean &amp;rsquo;there is&amp;rsquo;. Unlike English it remains singular even when followed by a plural:
Има котка - There is a cat Има много котки - There are many cats Няма котки - There are no cats References</description><content>&lt;p>The 3rd person singular of имам is also used to mean &amp;rsquo;there is&amp;rsquo;. Unlike English it remains singular even when followed by a plural:&lt;/p>
&lt;ul>
&lt;li>Има котка - There is a cat&lt;/li>
&lt;li>Има много котки - There are many cats&lt;/li>
&lt;li>Няма котки - There are no cats&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian To Have Or Not To Have</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-have-or-not-to-have/</link><pubDate>Wed, 15 Feb 2023 18:23:43 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-have-or-not-to-have/</guid><description>To Have Or Not To Have Имам is the verb &amp;rsquo;to have' Usually a verb can be negated using the не particle In the case of имам there is a specific verb: нямам Имам котка - I have a cat Иямам котка - I don&amp;rsquo;t have a cat Conjugation of both verbs at the present tense:
имам нямам I аз имам нямам you ти имаш нямаш he/she/it той/тя/то има няма we ние имаме нямаме you вие имате нямате they те имат нямат This is the standard example of conjugation of verbs of the 3rd group.</description><content>&lt;h2 id="to-have-or-not-to-have">To Have Or Not To Have&lt;/h2>
&lt;ul>
&lt;li>Имам is the verb &amp;rsquo;to have'&lt;/li>
&lt;li>Usually a verb can be negated using the не particle&lt;/li>
&lt;li>In the case of имам there is a specific verb: нямам&lt;/li>
&lt;li>Имам котка - I have a cat&lt;/li>
&lt;li>Иямам котка - I don&amp;rsquo;t have a cat&lt;/li>
&lt;/ul>
&lt;p>Conjugation of both verbs at the present tense:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>имам&lt;/th>
&lt;th>нямам&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>I&lt;/td>
&lt;td>аз&lt;/td>
&lt;td>имам&lt;/td>
&lt;td>нямам&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>you&lt;/td>
&lt;td>ти&lt;/td>
&lt;td>имаш&lt;/td>
&lt;td>нямаш&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>he/she/it&lt;/td>
&lt;td>той/тя/то&lt;/td>
&lt;td>има&lt;/td>
&lt;td>няма&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>we&lt;/td>
&lt;td>ние&lt;/td>
&lt;td>имаме&lt;/td>
&lt;td>нямаме&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>you&lt;/td>
&lt;td>вие&lt;/td>
&lt;td>имате&lt;/td>
&lt;td>нямате&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>they&lt;/td>
&lt;td>те&lt;/td>
&lt;td>имат&lt;/td>
&lt;td>нямат&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>This is the standard example of conjugation of verbs of the 3rd group. These are verbs that end in -ам, such as харесвам (to like) and искам (to want). The conjugation of these two and many others can be done in the same way:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>харесвам&lt;/th>
&lt;th>искам&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>I&lt;/td>
&lt;td>аз&lt;/td>
&lt;td>харесвам&lt;/td>
&lt;td>искам&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>you&lt;/td>
&lt;td>ти&lt;/td>
&lt;td>харесваш&lt;/td>
&lt;td>искаш&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>he/she/it&lt;/td>
&lt;td>той/тя/то&lt;/td>
&lt;td>харесва&lt;/td>
&lt;td>иска&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>we&lt;/td>
&lt;td>ние&lt;/td>
&lt;td>харесваме&lt;/td>
&lt;td>искаме&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>you&lt;/td>
&lt;td>вие&lt;/td>
&lt;td>харесвате&lt;/td>
&lt;td>искате&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>they&lt;/td>
&lt;td>те&lt;/td>
&lt;td>харесват&lt;/td>
&lt;td>искат&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian To Be</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-be/</link><pubDate>Wed, 15 Feb 2023 18:22:39 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-be/</guid><description> I am аз съм you are (familliar person) ти си he/she/it is той/тя/то е we are ние сме you are (talking to a group or one person to be formal) вие сте they are те са References</description><content>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>I am&lt;/th>
&lt;th>аз съм&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>you are (familliar person)&lt;/td>
&lt;td>ти си&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>he/she/it is&lt;/td>
&lt;td>той/тя/то е&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>we are&lt;/td>
&lt;td>ние сме&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>you are (talking to a group or one person to be formal)&lt;/td>
&lt;td>вие сте&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>they are&lt;/td>
&lt;td>те са&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Grammatical Gender</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-grammatical-gender/</link><pubDate>Wed, 15 Feb 2023 18:19:56 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-grammatical-gender/</guid><description>Masculine - Words ending in a consonant (including й) (e.g. мъж) Feminine - Words ending in -а or -я (e.g. жена) Neutral - Words ending in -о or -е (e.g. куче)
Words ending in -и, -у, -ю are generally loan words (often coming from English) and are usually neutral
меню такси Plurals Gender Ending Plural Masculine ‑consonant +и or +ове Feminine ‑а or ‑я ‑и Neutral ‑о or ‑е ‑a or +та Masculine Words и is added to words that have one syllable ове is added to words with more than one syllable Examples: Приятел - Приятели Град - Градове Мъж - Мъже (an exception) Feminine Words The final -а or -я is replaced with и</description><content>&lt;p>Masculine - Words ending in a consonant (including й) (e.g. мъж)
Feminine - Words ending in -а or -я (e.g. жена)
Neutral - Words ending in -о or -е (e.g. куче)&lt;/p>
&lt;p>Words ending in -и, -у, -ю are generally loan words (often coming from English) and are usually neutral&lt;/p>
&lt;ul>
&lt;li>меню&lt;/li>
&lt;li>такси&lt;/li>
&lt;/ul>
&lt;h2 id="plurals">Plurals&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Gender&lt;/th>
&lt;th>Ending&lt;/th>
&lt;th>Plural&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Masculine&lt;/td>
&lt;td>‑consonant&lt;/td>
&lt;td>+и or +ове&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Feminine&lt;/td>
&lt;td>‑а or ‑я&lt;/td>
&lt;td>‑и&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Neutral&lt;/td>
&lt;td>‑о or ‑е&lt;/td>
&lt;td>‑a or +та&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="masculine-words">Masculine Words&lt;/h3>
&lt;ul>
&lt;li>и is added to words that have one syllable&lt;/li>
&lt;li>ове is added to words with more than one syllable
Examples:&lt;/li>
&lt;li>Приятел - Приятели&lt;/li>
&lt;li>Град - Градове&lt;/li>
&lt;li>Мъж - Мъже (an exception)&lt;/li>
&lt;/ul>
&lt;h3 id="feminine-words">Feminine Words&lt;/h3>
&lt;p>The final -а or -я is replaced with и&lt;/p>
&lt;ul>
&lt;li>Жена - Жени&lt;/li>
&lt;li>Стая - Стаи&lt;/li>
&lt;/ul>
&lt;h3 id="neutral-words">Neutral Words&lt;/h3>
&lt;ul>
&lt;li>The final о is replaced by а&lt;/li>
&lt;li>The final e and for loan words, an extra та is added to the end:&lt;/li>
&lt;li>Село - Села&lt;/li>
&lt;li>Кафе - Кафета&lt;/li>
&lt;li>Такси - Таксита&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Negation</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-negation/</link><pubDate>Wed, 15 Feb 2023 18:17:47 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-negation/</guid><description>Negation &amp;ldquo;Не&amp;rdquo; means no and it can be used to negate a sentence:
&amp;ldquo;Ана не е мъж&amp;rdquo; - Ana is not a man References</description><content>&lt;h2 id="negation">Negation&lt;/h2>
&lt;p>&amp;ldquo;Не&amp;rdquo; means no and it can be used to negate a sentence:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Ана не е мъж&amp;rdquo; - Ana is not a man&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Indefinite Article</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/indefinite-article/</link><pubDate>Mon, 30 Jan 2023 21:29:38 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/indefinite-article/</guid><description>An indefinite article refers to non-specific nouns. You could say:
I need a pen.
I want an orange.
In both cases, this does not refer to a specific pen or orange. Any will do.
References</description><content>&lt;p>An indefinite article refers to non-specific &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">nouns&lt;/a>. You could say:&lt;/p>
&lt;blockquote>
&lt;p>I need &lt;strong>a&lt;/strong> pen.&lt;/p>
&lt;p>I want &lt;strong>an&lt;/strong> orange.&lt;/p>
&lt;/blockquote>
&lt;p>In both cases, this does not refer to a specific pen or orange. Any will do.&lt;/p>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Conjugation</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/conjugation/</link><pubDate>Sun, 29 Jan 2023 22:05:08 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/conjugation/</guid><description>A verb can be conjugated with reference to the number, person, mood or tense. Examples:
I teach English We teach English You teach English He teaches English She teaches English They teach English References</description><content>&lt;p>A &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a> can be conjugated with reference to the number, person, mood or tense. Examples:&lt;/p>
&lt;ul>
&lt;li>I &lt;em>teach&lt;/em> English&lt;/li>
&lt;li>We &lt;em>teach&lt;/em> English&lt;/li>
&lt;li>You &lt;em>teach&lt;/em> English&lt;/li>
&lt;li>He &lt;em>teaches&lt;/em> English&lt;/li>
&lt;li>She &lt;em>teaches&lt;/em> English&lt;/li>
&lt;li>They &lt;em>teach&lt;/em> English&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Citation Form</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/citation-form/</link><pubDate>Sun, 29 Jan 2023 22:03:29 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/citation-form/</guid><description>In Bulgarian, citation form is used to refer to a verb or to look it up in a dictionary. It is different from the infinitive in a grammatical sense.
For example:
&amp;ldquo;to be&amp;rdquo; is &amp;ldquo;съм&amp;rdquo; Съм is used for the citation form and for &amp;ldquo;I am&amp;rdquo; References</description><content>&lt;p>In Bulgarian, citation form is used to refer to a &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a> or to look it up in a dictionary. It is different from the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/infinitive/">infinitive&lt;/a> in a grammatical sense.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;to be&amp;rdquo; is &amp;ldquo;съм&amp;rdquo;&lt;/li>
&lt;li>Съм is used for the citation form and for &amp;ldquo;I am&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Adverb</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/adverb/</link><pubDate>Sun, 29 Jan 2023 21:46:03 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/adverb/</guid><description>An adverb is a word that modifies a verb (he sings loudly), an adjective (very tall), another adverb or even a whole sentence (fortunately, I had brought an umbrella).
References</description><content>&lt;p>An adverb is a word that modifies a &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a> (&lt;em>he sings&lt;/em> &lt;em>&lt;strong>loudly&lt;/strong>&lt;/em>), an &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/adjective/">adjective&lt;/a> (&lt;em>&lt;strong>very&lt;/strong>&lt;/em> &lt;em>tall&lt;/em>), another &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/adverb/">adverb&lt;/a> or even a whole sentence (&lt;em>&lt;strong>fortunately&lt;/strong>&lt;/em>, &lt;em>I had brought an umbrella&lt;/em>).&lt;/p>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Infinitive</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/infinitive/</link><pubDate>Sun, 29 Jan 2023 21:41:14 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/infinitive/</guid><description>Infinitives are a special form of verbs that can be used as a noun, adjective or adverb. They are usually made by adding the word to before the base verb. They are useful when discussion actions without actually doing the action:
I want to go home. I like to write in English. Example:
I need to win. Today, we win. In the first sentence, the infinitive form of the verb win is used.</description><content>&lt;p>Infinitives are a special form of &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verbs&lt;/a> that can be used as a &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">noun&lt;/a>, &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/adjective/">adjective&lt;/a> or &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/adverb/">adverb&lt;/a>. They are usually made by adding the word &lt;em>to&lt;/em> before the base verb. They are useful when discussion actions without actually doing the action:&lt;/p>
&lt;ul>
&lt;li>I want to go home.&lt;/li>
&lt;li>I like to write in English.&lt;/li>
&lt;/ul>
&lt;p>Example:&lt;/p>
&lt;ul>
&lt;li>I need &lt;strong>to win&lt;/strong>.&lt;/li>
&lt;li>Today, we win.&lt;/li>
&lt;/ul>
&lt;p>In the first sentence, the infinitive form of the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a> &lt;em>win&lt;/em> is used. The main &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a> of the sentence is actually &amp;ldquo;need&amp;rdquo;. The second sentence uses the standard form of &lt;em>win&lt;/em> as an actionable &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a>. In he first sentence with the infinitive, the action of &amp;ldquo;winning&amp;rdquo; is not actually done. The sentence simply discusses the idea of winning. The second sentence however, describes the action of winning.&lt;/p>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Verb</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/verb/</link><pubDate>Sun, 29 Jan 2023 21:41:14 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/verb/</guid><description>A verb is a word that describes what the subject of the sentence is doing. They can indicate (physical or mental) actions, occurrences and states of being.
Jeff builds a house.
Anita is thinking about horses.
True love exists.
References</description><content>&lt;p>A verb is a word that describes what the subject of the sentence is doing. They can indicate (physical or mental) actions, occurrences and states of being.&lt;/p>
&lt;blockquote>
&lt;p>Jeff &lt;strong>builds&lt;/strong> a house.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Anita &lt;strong>is thinking&lt;/strong> about horses.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>True love &lt;strong>exists&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Conjugation</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-conjugation/</link><pubDate>Sun, 29 Jan 2023 21:39:05 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-conjugation/</guid><description>In Bulgarian verbs are conjugated according to the subject (I, you, &amp;hellip;) and the tense (past, present, &amp;hellip;). This conjugation is applied to the ending of the verb.
References</description><content>&lt;p>In Bulgarian &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verbs&lt;/a> are &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/conjugation/">conjugated&lt;/a> according to the subject (I, you, &amp;hellip;) and the tense (past, present, &amp;hellip;). This conjugation is applied to the ending of the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a>.&lt;/p>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Starting Sentences With A Present Form</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-starting-sentences-with-a-present-form/</link><pubDate>Sun, 29 Jan 2023 21:39:05 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-starting-sentences-with-a-present-form/</guid><description>It is a rule that you cannot start a sentence with a present form of the verb съм (&amp;ldquo;to be&amp;rdquo;). When you omit the subject personal pronoun, the verb has to move into the second position:
&amp;ldquo;Аз съм мъж&amp;rdquo; or &amp;ldquo;Мъж съм&amp;rdquo; – I am a man It is incorrect to say: &amp;ldquo;съм мъж&amp;rdquo;
References</description><content>&lt;p>It is a rule that you cannot start a sentence with a present form of the verb &lt;em>съм&lt;/em> (&amp;ldquo;to be&amp;rdquo;). When you omit the subject personal &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/pronoun/">pronoun&lt;/a>, the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a> has to move into the second position:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Аз съм мъж&amp;rdquo; or &amp;ldquo;Мъж съм&amp;rdquo; – I am a man&lt;/li>
&lt;/ul>
&lt;p>It is incorrect to say: &amp;ldquo;съм мъж&amp;rdquo;&lt;/p>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Verbs</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-verbs/</link><pubDate>Sun, 29 Jan 2023 21:39:05 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-verbs/</guid><description>Verbs do not have an infinitive form. When referring to a verb, use the first person conjugation of that verb. Here are examples Bulgarian the citation form &amp;ldquo;to be&amp;rdquo;.
Example:
&amp;ldquo;to be&amp;rdquo; is &amp;ldquo;съм&amp;rdquo; Съм is used for the citation form and for &amp;ldquo;I am&amp;rdquo; Two conjugations of &amp;ldquo;съм&amp;rdquo;:
I am - Аз съм he/she/it is - той/тя/то е Examples:
&amp;ldquo;Той е мъж&amp;rdquo; - He is a man &amp;ldquo;Тя е жена&amp;rdquo; - She is a woman &amp;ldquo;Това е куче&amp;rdquo; - This is a dog Since the conjugations are unique to each subject, the pronoun can be left out:</description><content>&lt;p>Verbs do not have an &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/infinitive/">infinitive&lt;/a> form. When referring to a &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a>, use the first person &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/conjugation/">conjugation&lt;/a> of that &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">verb&lt;/a>. Here are examples Bulgarian the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/citation-form/">citation form&lt;/a> &amp;ldquo;to be&amp;rdquo;.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;to be&amp;rdquo; is &amp;ldquo;съм&amp;rdquo;&lt;/li>
&lt;li>Съм is used for the citation form and for &amp;ldquo;I am&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>Two conjugations of &amp;ldquo;съм&amp;rdquo;:&lt;/p>
&lt;ul>
&lt;li>I am - Аз съм&lt;/li>
&lt;li>he/she/it is - той/тя/то е&lt;/li>
&lt;/ul>
&lt;p>Examples:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Той е мъж&amp;rdquo; - He is a man&lt;/li>
&lt;li>&amp;ldquo;Тя е жена&amp;rdquo; - She is a woman&lt;/li>
&lt;li>&amp;ldquo;Това е куче&amp;rdquo; - This is a dog&lt;/li>
&lt;/ul>
&lt;p>Since the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/conjugation/">conjugations&lt;/a> are unique to each subject, the &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/pronoun/">pronoun&lt;/a> can be left out:&lt;/p>
&lt;ul>
&lt;li>Instead of &amp;ldquo;аз съм мъж&amp;rdquo; (I am a man), you can say &amp;ldquo;мъж съм&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Bulgarian Articles</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-articles/</link><pubDate>Sun, 29 Jan 2023 21:18:20 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-articles/</guid><description>In English there are 2 types of articles:
Definite Indefinite There is no definite article in Bulgarian: &amp;ldquo;жена&amp;rdquo; and &amp;ldquo;куче&amp;rdquo; are used alone to refer to &amp;ldquo;a woman&amp;rdquo; and &amp;ldquo;a dog&amp;rdquo;: &amp;ldquo;Ту е жена&amp;rdquo; - She is a woman &amp;ldquo;Аз имам куче&amp;rdquo; - I have a dog References</description><content>&lt;p>In English there are 2 types of articles:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/definite-article/">Definite&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/indefinite-article/">Indefinite&lt;/a>
There is no &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/definite-article/">definite article&lt;/a> in Bulgarian:&lt;/li>
&lt;li>&amp;ldquo;жена&amp;rdquo; and &amp;ldquo;куче&amp;rdquo; are used alone to refer to &amp;ldquo;a woman&amp;rdquo; and &amp;ldquo;a dog&amp;rdquo;:&lt;/li>
&lt;li>&amp;ldquo;Ту е жена&amp;rdquo; - She is a woman&lt;/li>
&lt;li>&amp;ldquo;Аз имам куче&amp;rdquo; - I have a dog&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Adjective</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/adjective/</link><pubDate>Sun, 29 Jan 2023 21:09:11 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/adjective/</guid><description>Adjectives are words that describe the qualities or states of being of nouns. Examples:
Rob wore a beautiful hat.
Furry dogs may overhead in the summertime.
My cake should have sixteen candles.
The scariest villain of all time is Darth Vader.
References</description><content>&lt;p>Adjectives are words that describe the qualities or states of being of &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">nouns&lt;/a>. Examples:&lt;/p>
&lt;blockquote>
&lt;p>Rob wore a &lt;strong>beautiful&lt;/strong> hat.&lt;/p>
&lt;p>&lt;strong>Furry&lt;/strong> dogs may overhead in the summertime.&lt;/p>
&lt;p>My cake should have &lt;strong>sixteen&lt;/strong> candles.&lt;/p>
&lt;p>The &lt;strong>scariest&lt;/strong> villain of all time is Darth Vader.&lt;/p>
&lt;/blockquote>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Noun</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/noun/</link><pubDate>Sun, 29 Jan 2023 21:09:11 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/noun/</guid><description>A noun is a word that refers to a person, place, thing or idea.
References</description><content>&lt;p>A noun is a word that refers to a person, place, thing or idea.&lt;/p>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Pronoun</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/pronoun/</link><pubDate>Sun, 29 Jan 2023 21:09:11 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/pronoun/</guid><description>A pronoun is a word that stands in a for noun, often to avoid the need to repeat the same noun over and over. Use this when your reader/listener already knows which nouns you are referring to.
Example:
I have a dog. He&amp;rsquo;s brown and white. References</description><content>&lt;p>A pronoun is a word that stands in a for &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">noun&lt;/a>, often to avoid the need to repeat the same &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">noun&lt;/a> over and over. Use this when your reader/listener already knows which &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">nouns&lt;/a> you are referring to.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;ul>
&lt;li>I have a dog. He&amp;rsquo;s brown and white.&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2></content></item><item><title>Definite Article</title><link>https://www.okwilkins.dev/knowledge-system/slip-box/definite-article/</link><pubDate>Sun, 29 Jan 2023 21:09:06 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/slip-box/definite-article/</guid><description>A definite article refers to a specific noun. You could say:
The man&amp;rsquo;s dog barked at me.
I lent my sister the book when I finished reading it.
I didn&amp;rsquo;t go to the the party.
References</description><content>&lt;p>A definite article refers to a specific &lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">noun&lt;/a>. You could say:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>The&lt;/strong> man&amp;rsquo;s dog barked at me.&lt;/p>
&lt;p>I lent my sister &lt;strong>the&lt;/strong> book when I finished reading it.&lt;/p>
&lt;p>I didn&amp;rsquo;t go to the &lt;strong>the&lt;/strong> party.&lt;/p>
&lt;/blockquote>
&lt;h2 id="references">References&lt;/h2></content></item><item><title/><link>https://www.okwilkins.dev/knowledge-system/map-of-content/map-of-content/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/map-of-content/map-of-content/</guid><description> Bulgarian Data Science Language Programming</description><content>&lt;ul>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/map-of-content/bulgarian/">Bulgarian&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/map-of-content/data-science/">Data Science&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/map-of-content/language/">Language&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.okwilkins.dev/knowledge-system/map-of-content/programming/">Programming&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Bulgarian</title><link>https://www.okwilkins.dev/knowledge-system/map-of-content/bulgarian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/map-of-content/bulgarian/</guid><description>Bulgarian Articles Bulgarian Closed Questions Bulgarian Conjugation Bulgarian Counting To 10 Bulgarian Definite Articles Bulgarian Grammatical Gender Bulgarian Masculine Definite Articles Bulgarian Negation Bulgarian Starting Sentences with a Present Form Bulgarian There Is Bulgarian To Be Bulgarian To Have Or Not To Have Bulgarian Verbs Citation Form Numeral Plural</description><content>&lt;p>&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-articles/">Bulgarian Articles&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-closed-questions/">Bulgarian Closed Questions&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-conjugation/">Bulgarian Conjugation&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-counting-to-10/">Bulgarian Counting To 10&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-definite-articles/">Bulgarian Definite Articles&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-grammatical-gender/">Bulgarian Grammatical Gender&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-masculine-definite-articles/">Bulgarian Masculine Definite Articles&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-negation/">Bulgarian Negation&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-starting-sentences-with-a-present-form/">Bulgarian Starting Sentences with a Present Form&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-there-is/">Bulgarian There Is&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-be/">Bulgarian To Be&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-have-or-not-to-have/">Bulgarian To Have Or Not To Have&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-verbs/">Bulgarian Verbs&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/citation-form/">Citation Form&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/numeral-plural/">Numeral Plural&lt;/a>&lt;/p></content></item><item><title>Data Science</title><link>https://www.okwilkins.dev/knowledge-system/map-of-content/data-science/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/map-of-content/data-science/</guid><description>Decision Trees Machine Learning Bias Machine Learning Variance</description><content>&lt;p>&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/decision-trees/">Decision Trees&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/machine-learning-bias/">Machine Learning Bias&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/machine-learning-variance/">Machine Learning Variance&lt;/a>&lt;/p></content></item><item><title>Language</title><link>https://www.okwilkins.dev/knowledge-system/map-of-content/language/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/map-of-content/language/</guid><description>Bulgarian Adjective Adverb Bulgarian Articles Bulgarian Closed Questions Bulgarian Conjugation Bulgarian Counting To 10 Bulgarian Definite Articles Bulgarian Grammatical Gender Bulgarian Masculine Definite Articles Bulgarian Negation Bulgarian Starting Sentences with a Present Form Bulgarian There Is Bulgarian To Be Bulgarian To Have Or Not To Have Citation Form Conjugation Definite Article Indefinite Article Infinitive Noun Numeral Plural Pronoun Verb</description><content>&lt;p>&lt;a href="https://www.okwilkins.dev/knowledge-system/map-of-content/bulgarian/">Bulgarian&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/adjective/">Adjective&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/adverb/">Adverb&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-articles/">Bulgarian Articles&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-closed-questions/">Bulgarian Closed Questions&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-conjugation/">Bulgarian Conjugation&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-counting-to-10/">Bulgarian Counting To 10&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-definite-articles/">Bulgarian Definite Articles&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-grammatical-gender/">Bulgarian Grammatical Gender&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-masculine-definite-articles/">Bulgarian Masculine Definite Articles&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-negation/">Bulgarian Negation&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-starting-sentences-with-a-present-form/">Bulgarian Starting Sentences with a Present Form&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-there-is/">Bulgarian There Is&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-be/">Bulgarian To Be&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/bulgarian-to-have-or-not-to-have/">Bulgarian To Have Or Not To Have&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/citation-form/">Citation Form&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/conjugation/">Conjugation&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/definite-article/">Definite Article&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/indefinite-article/">Indefinite Article&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/infinitive/">Infinitive&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/noun/">Noun&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/numeral-plural/">Numeral Plural&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/pronoun/">Pronoun&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/verb/">Verb&lt;/a>&lt;/p></content></item><item><title>Mermaid</title><link>https://www.okwilkins.dev/knowledge-system/references/mermaid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/references/mermaid/</guid><description>About Mermaid &amp;ldquo;About Mermaid&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/intro/ Flowcharts &amp;ldquo;Flowcharts - Basic Syntax&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/flowchart.html Sequence Diagrams &amp;ldquo;Sequence diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/sequenceDiagram.html Class Diagrams &amp;ldquo;Class diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/classDiagram.html State Diagrams &amp;ldquo;State diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/stateDiagram.html Entity Relationship Diagrams &amp;ldquo;Entity Relationship Diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/entityRelationshipDiagram.html User Journey Diagrams &amp;ldquo;User Journey Diagram&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/userJourney.html Gantt Charts &amp;ldquo;Gantt diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/gantt.html Pie Charts &amp;ldquo;Pie chart diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.js.org/syntax/pie.html Gitgraph Diagrams &amp;ldquo;Gitgraph Diagrams&amp;rdquo;, MermaidJS, 2023-02-27, https://mermaid.</description><content>&lt;h2 id="about-mermaid">About Mermaid&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;About Mermaid&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/intro/">https://mermaid.js.org/intro/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="flowcharts">Flowcharts&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Flowcharts - Basic Syntax&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/flowchart.html">https://mermaid.js.org/syntax/flowchart.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="sequence-diagrams">Sequence Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Sequence diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/sequenceDiagram.html">https://mermaid.js.org/syntax/sequenceDiagram.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="class-diagrams">Class Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Class diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/classDiagram.html">https://mermaid.js.org/syntax/classDiagram.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="state-diagrams">State Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;State diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/stateDiagram.html">https://mermaid.js.org/syntax/stateDiagram.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="entity-relationship-diagrams">Entity Relationship Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Entity Relationship Diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/entityRelationshipDiagram.html">https://mermaid.js.org/syntax/entityRelationshipDiagram.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="user-journey-diagrams">User Journey Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;User Journey Diagram&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/userJourney.html">https://mermaid.js.org/syntax/userJourney.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gantt-charts">Gantt Charts&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Gantt diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/gantt.html">https://mermaid.js.org/syntax/gantt.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="pie-charts">Pie Charts&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Pie chart diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/pie.html">https://mermaid.js.org/syntax/pie.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gitgraph-diagrams">Gitgraph Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Gitgraph Diagrams&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/gitgraph.html">https://mermaid.js.org/syntax/gitgraph.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="mindmap">Mindmap&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Mindmap&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/mindmap.html">https://mermaid.js.org/syntax/mindmap.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="timeline-diagrams">Timeline Diagrams&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Timeline Diagram&amp;rdquo;, &lt;em>MermaidJS&lt;/em>, 2023-02-27, &lt;a href="https://mermaid.js.org/syntax/timeline.html">https://mermaid.js.org/syntax/timeline.html&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Programming</title><link>https://www.okwilkins.dev/knowledge-system/map-of-content/programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/map-of-content/programming/</guid><description>Mermaid Class Diagram Mermaid Entity Relationship Diagram Mermaid Gitgraph Diagram</description><content>&lt;p>&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-class-diagram/">Mermaid Class Diagram&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-entity-relationship-diagram/">Mermaid Entity Relationship Diagram&lt;/a>
&lt;a href="https://www.okwilkins.dev/knowledge-system/slip-box/mermaid-gitgraph-diagram/">Mermaid Gitgraph Diagram&lt;/a>&lt;/p></content></item><item><title>Statquest With Josh Starmer</title><link>https://www.okwilkins.dev/knowledge-system/references/statquest-with-josh-starmer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.okwilkins.dev/knowledge-system/references/statquest-with-josh-starmer/</guid><description>Bias &amp;ldquo;Bias and Variance&amp;rdquo;, Youtube, StatQuest with Josh Starmer, 2023-02-27, https://youtu.be/EuBBz3bI-aA?t=29 Variance &amp;ldquo;Bias and Variance&amp;rdquo;, Youtube, StatQuest with Josh Starmer, 2023-02-27, https://youtu.be/EuBBz3bI-aA?t=246 Decision Trees &amp;ldquo;Decision and Classification Trees, Clearly Explained!!!&amp;rdquo;, Youtube, StatQuest with Josh Starmer, 2023-02-27, https://www.youtube.com/watch?v=_L39rN6gz7Y &amp;ldquo;Regression Trees, Clearly Explained!!!&amp;rdquo;, Youtube, StatQuest with Josh Starmer, 2023-02-27, https://www.youtube.com/watch?v=g9c66TUylZ4 &amp;ldquo;How to Prune Regression Trees, Clearly Explained!!!&amp;rdquo; Youtube, StatQuest with Josh Starmer, 2023-02-27, https://www.youtube.com/watch?v=D0efHEJsfHo</description><content>&lt;h2 id="bias">Bias&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Bias and Variance&amp;rdquo;, &lt;em>Youtube&lt;/em>, &lt;em>StatQuest with Josh Starmer&lt;/em>, 2023-02-27, &lt;a href="https://youtu.be/EuBBz3bI-aA?t=29">https://youtu.be/EuBBz3bI-aA?t=29&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="variance">Variance&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Bias and Variance&amp;rdquo;, &lt;em>Youtube&lt;/em>, &lt;em>StatQuest with Josh Starmer&lt;/em>, 2023-02-27, &lt;a href="https://youtu.be/EuBBz3bI-aA?t=246">https://youtu.be/EuBBz3bI-aA?t=246&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="decision-trees">Decision Trees&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Decision and Classification Trees, Clearly Explained!!!&amp;rdquo;, &lt;em>Youtube&lt;/em>, &lt;em>StatQuest with Josh Starmer&lt;/em>, 2023-02-27, &lt;a href="https://www.youtube.com/watch?v=">https://www.youtube.com/watch?v=&lt;/a>_L39rN6gz7Y&lt;/li>
&lt;li>&amp;ldquo;Regression Trees, Clearly Explained!!!&amp;rdquo;, &lt;em>Youtube&lt;/em>, &lt;em>StatQuest with Josh Starmer&lt;/em>, 2023-02-27, &lt;a href="https://www.youtube.com/watch?v=g9c66TUylZ4">https://www.youtube.com/watch?v=g9c66TUylZ4&lt;/a>&lt;/li>
&lt;li>&amp;ldquo;How to Prune Regression Trees, Clearly Explained!!!&amp;rdquo; &lt;em>Youtube&lt;/em>, &lt;em>StatQuest with Josh Starmer&lt;/em>, 2023-02-27, &lt;a href="https://www.youtube.com/watch?v=D0efHEJsfHo">https://www.youtube.com/watch?v=D0efHEJsfHo&lt;/a>&lt;/li>
&lt;/ul></content></item></channel></rss>